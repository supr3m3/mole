{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to Mole \u00b6 Mole is a tool to assist with testing and experimentation involving robots and autonomous systems. It can facilitate the following: test and experiment execution : tracking who, what, where, when, why, and how of the experiment data collection : acquiring targeted data based on events representing \"conditions of interest\" orchestration : controlling experiment infrastructure to enable dyamic yet repeatable scenarios monitoring : confirming system, infrastructure, and personnel status over time analysis and reporting : generating pre-defined performance metrics and figures to rapidly produce stakeholder reports Mole originated as a way to collect data for uncrewed ground vehicles and to automatically generate reports with summary metrics and figures. It now finds a number of additional use cases spanning single and multi-vehicle operations within air, ground, and sea domains -- both live and virtual. This documentation seeks to provide sufficient background to enable using Mole or contributing to its development.","title":"About"},{"location":"index.html#welcome-to-mole","text":"Mole is a tool to assist with testing and experimentation involving robots and autonomous systems. It can facilitate the following: test and experiment execution : tracking who, what, where, when, why, and how of the experiment data collection : acquiring targeted data based on events representing \"conditions of interest\" orchestration : controlling experiment infrastructure to enable dyamic yet repeatable scenarios monitoring : confirming system, infrastructure, and personnel status over time analysis and reporting : generating pre-defined performance metrics and figures to rapidly produce stakeholder reports Mole originated as a way to collect data for uncrewed ground vehicles and to automatically generate reports with summary metrics and figures. It now finds a number of additional use cases spanning single and multi-vehicle operations within air, ground, and sea domains -- both live and virtual. This documentation seeks to provide sufficient background to enable using Mole or contributing to its development.","title":"Welcome to Mole"},{"location":"basic_config.html","text":"Configuration \u00b6 First Steps \u00b6 If we're starting with a blank slate, we have a couple initial steps to go through to configure Django. We need to create a series of Django model instances to tailor the Mole instrumentation to a specific domain. There are two ways to create instances: using the Django ORM or using third-party library factory_boy . The main difference between the two is how much data must be specified. The ORM requires all the fields to be filled out but factory_boy is configured to fill in fields that are left blank. Regardless of which is chosen, this should be implemented in a custom Django command. An example of one can be found at mole/data_collection/management/commands/configure_mole.py . Locations \u00b6 The first thing we need to create is a location. Django ORM Factory Boy import data_collection.models as dcm location_1 = dcm . Location . objects . create ( name = \"Location 1\" , description = \"description of location 1\" , point = \"POINT(-117.248 32.709)\" , timezone = \"America/Los_Angeles\" , ) from data_collection.factories import factories location_1 = factories . LocationFactory ( name = \"Location 1\" , description = \"description of location 1\" , point = \"POINT(-117.248 32.709)\" , timezone = \"America/Los_Angeles\" , ) The name and description fields are self-explanatory. The point field is a well-known text representation (WKT) of a point geometry. The timezone field is a string indicating the TZ database name. Campaigns \u00b6 Next we need to create a campaign. A campaign can represent a high level field experiment or test event. Django ORM Factory Boy import data_collection.models as dcm campaign_1 = dcm . Campaign . objects . create ( name = \"Test Event 1\" , description = \"info about test event 1\" , start_datetime = \"2018-10-21T05:00:00-0800\" , end_datetime = \"2018-10-25T19:00:00-0800\" , location = location_1 , trial_id_major_name = \"Day\" , trial_id_minor_name = \"Shift\" , trial_id_micro_name = \"Attempt\" , ) from data_collection.factories import factories campaign_1 = factories . CampaignFactory ( name = \"Test Event 1\" , description = \"info about test event 1\" , start_datetime = \"2018-10-21T05:00:00-0800\" , end_datetime = \"2018-10-25T19:00:00-0800\" , location = location_1 , trial_id_major_name = \"Day\" , trial_id_minor_name = \"Shift\" , trial_id_micro_name = \"Attempt\" , ) start_datetime and end_datetime indicate the start and end time respectively. location should be a reference to the Location object created earlier. trial_id_major_name , trial_id_minor_name , and trial_id_micro_name are strings that describe the numbering scheme for trials. Each trial has a major, minor, and micro id, visualized as x.y.z . For example, the following code block marks the x id as the day. These strings are optional and will default to the empty string if not specified. Scenarios \u00b6 Next we'll create a Scenario. But first we need to create a Test Method. A Test Method represents a defined procedure to test a capability. It could be focused on a low level capability such as obstacle detection or path planning, or a system-level test that is intended to exercise the fully-integrated system. A Scenario is a specific instantiation of a Test Method that defines parameters that may be variable in the Test Method (e.g., location, number of agents, test duration, dynamic elements, etc.). So the Test Method might be \"Persistent Swarm Surveillance\", and the Scenario might be \" 3-agent surveillance\". This is intended to allow association of similar Test Methods while allowing some variation in location or parameters. Django ORM Factory Boy import data_collection.models as dcm test_method = dcm . TestMethod . objects . create ( name = \"Interactive Swarm Exercise\" , description = \"Testing swarm capabilities\" , version_major = 1 , version_minor = 0 , version_micro = 0 , variant = \"autonomy\" , ) from data_collection.factories import factories test_method = factories . TestMethodFactory ( name = \"Interactive Swarm Exercise\" , description = \"Testing swarm capabilities\" , version_major = 1 , version_minor = 0 , version_micro = 0 , variant = \"autonomy\" , ) The version numbers and variant can be used to track changes to test procedures. Now we can create the scenario. Django ORM Factory Boy import data_collection.models as dcm scenario_1 = dcm . Scenario . objects . create ( name = \"Scenario 1\" , description = \"description of scenario 1\" , location = location_1 , test_method = test_method , ) from data_collection.factories import factories scenario_1 = factories . ScenarioFactory ( name = \"Scenario 1\" , description = \"description of scenario 1\" , location = location_1 , test_method = test_method , ) The scenario's location could be the same or different than the campaign's location. If different, follow the steps for creating a location to create another one. Trials \u00b6 Now we move on to our trials. A Trial can represent a single run or attempt. Events of interest or data about the run can be saved in these trials, giving us an easy container to compare and contrast test results. Since trials contain a lot of data about a run, we'll need to create a couple of pre-requisite instances. The first is a Tester which needs both a user and a role. We use the Django authentication user so unlike the other models, we have to use the factory_boy implementation to ensure that the password is saved correctly. Factory Boy from data_collection.factories import factories main_user = factories . UserFactory ( username = \"admin\" , password = \"admin\" , ) Warning You'll want to adjust this to have more secure credentials. Next we'll create the role. Django ORM Factory Boy import data_collection.models as dcm my_role = dcm . Role . objects . create ( name = \"test_administrator\" , description = \"description of test administrator\" , ) from data_collection.factories import factories my_role = factories . RoleFactory ( name = \"test_administrator\" , description = \"description of test administrator\" , ) Now that we have both a user and a role, we can create a tester. Django ORM Factory Boy import data_collection.models as dcm my_tester = dcm . Tester . objects . create ( user = main_user , role = my_role , ) from data_collection.factories import factories my_tester = factories . TesterFactory ( user = main_user , role = my_role , ) Next we need a system configuration. A system configuration can consist of one or more capability under test, each of which has a performer. We'll start at the bottom with the performer. Django ORM Factory Boy import data_collection.models as dcm performer_1 = dcm . Performer . objects . create ( name = \"Integrator Team 1\" , description = \"info about team 1\" , ) from data_collection.factories import factories performer_1 = factories . PerformerFactory ( name = \"Integrator Team 1\" , description = \"info about team 1\" , ) Next up is capability under test. Django ORM Factory Boy import data_collection.models as dcm capability_under_test = dcm . CapabilityUnderTest . objects . create ( name = \"Swarm Capability\" , description = \"Swarm Capability\" , performer = performer_1 , ) from data_collection.factories import factories capability_under_test = factories . CapabilityUnderTestFactory ( name = \"Swarm Capability\" , description = \"Swarm Capability\" , performer = performer_1 , ) Then we round it out with a system configuration. Django ORM Factory Boy import data_collection.models as dcm sys_conf = dcm . SystemConfiguration . objects . create ( name = \"System Configuration 1\" , description = \"total set of performers\" , capabilities_under_test = ( capability_under_test ,), ) from data_collection.factories import factories sys_conf = factories . SystemConfigurationFactory ( name = \"System Configuration 1\" , description = \"total set of performers\" , capabilities_under_test = ( capability_under_test ,), ) Next we have to create a test condition, as well as a weather instance. First we'll start with the weather. Django ORM Factory Boy import data_collection.models as dcm current_weather = dcm . Weather . objects . create ( name = \"Sunny\" , description = \"no clouds in the sky\" , ) from data_collection.factories import factories current_weather = factories . WeatherFactory ( name = \"Sunny\" , description = \"no clouds in the sky\" , ) Then onto the test condition. Django ORM Factory Boy import data_collection.models as dcm my_test_condition = dcm . TestCondition . objects . create ( weather = current_weather , ) from data_collection.factories import factories my_test_condition = factories . TestConditionFactory ( weather = current_weather , ) Now we can finally create an initial trial. Django ORM Factory Boy import data_collection.models as dcm dcm . Trial . objects . create ( id_major = 0 , id_minor = 0 , id_micro = 0 , campaign = campaign_1 , scenario = scenario_1 , testers = ( my_tester ,), test_condition = my_test_condition , system_configuration = sys_conf , start_datetime = \"2018-10-22T07:00:00-0800\" , reported = False , ) from data_collection.factories import factories factories . TrialFactory ( id_major = 0 , id_minor = 0 , id_micro = 0 , campaign = campaign_1 , scenario = scenario_1 , testers = ( my_tester ,), test_condition = my_test_condition , system_configuration = sys_conf , start_datetime = \"2018-10-22T07:00:00-0800\" , reported = False , ) The ternary id ( id_major , id_minor , id_micro taken together) have to be unique within each campaign, but there is no restriction otherwise on their integer values. reported is a boolean flag that will tell the report generator whether or not to create graphs and charts for this trial. Event Types \u00b6 With a trial in place, we can start creating event types to track the things we are interested in. These event types can represent 'bookmarks' during the test run or a change in state. It can be used to show an interaction of interest or a noteworthy exchange. We'll first create an event level. Django ORM Factory Boy import data_collection.models as dcm event_level = dcm . EventLevel . objects . create ( name = \"Info\" , description = \"Informational events\" , key = \"info\" , visibility = 1 , ) from data_collection.factories import factories event_level = factories . EventLevelFactory ( name = \"Info\" , description = \"Informational events\" , key = \"info\" , visibility = 1 , ) The name and key fields are both unique strings and the visibility is set to some integer value, used to dictate a hierarchy of event types. Now we move on to the event type itself. Django ORM Factory Boy import data_collection.models as dcm dcm . EventType . objects . create ( name = \"Interaction\" , description = \"interaction of note\" , event_level = event_level , is_manual = True , has_duration = False , ) from data_collection.factories import factories factories . EventTypeFactory ( name = \"Interaction\" , description = \"interaction of note\" , event_level = event_level , is_manual = True , has_duration = False , ) Entities \u00b6 This model provides an abstraction for any unit or system we wish to track. This could be a system that's undergoing testing or an orchestration element that we're configuring. Each entity will need an entity type. We can also assign a list of capabilities to an entity type, signifying that every entity of this type has these properties. We can further distinguish individual entities by assigning mods to them, where mods can have multiple capabilities. We'll start with defining a couple of capabilities. Django ORM Factory Boy import data_collection.models as dcm radar_cap = dcm . Capability . objects . create ( name = \"radar\" , description = \"24 GHz Radar\" , display_name = \"Radar\" , ) ble_cap = dcm . Capability . objects . create ( name = \"ble\" , description = \"Capability using Bluetooth Low Energy\" , display_name = \"Bluetooth Low Energy\" , ) from data_collection.factories import factories radar_cap = factories . CapabilityFactory ( name = \"radar\" , description = \"Radar Capability\" , display_name = \"Radar\" , ) ble_cap = factories . CapabilityFactory ( name = \"ble\" , description = \"Capability using Bluetooth Low Energy\" , display_name = \"Bluetooth Low Energy\" , ) display_name is a more human-friendly string that can be used for a UI for example. Next we'll make a mod combining the two capabilities. Django ORM Factory Boy import data_collection.models as dcm comms_mod = dcm . Mod . objects . create ( name = \"comms_mod\" , description = \"A combination mod that contains both radar and bluetooth low energy\" , display_name = \"Combined communication mod\" , ) comms_mod . capabilities . set ([ radar_cap , ble_cap ]) from data_collection.factories import factories comms_mod = factories . ModFactory ( name = \"comms_mod\" , description = \"A combination mod that contains both radar and bluetooth low energy\" , display_name = \"Combined communication mod\" , capabilities = [ radar_cap , ble_cap ], ) Note that when using the Django ORM, the process to add a capability to a mod is slightly more complicated than the creation of the mod. It must be set outside of the creation of the mod due to technical limitations. This occurs in various other models throughout Mole as well. This limitation does not occur in the factory_boy factories. We'll make an entity type next, a ugv entity type with an inherent radar capability. Django ORM Factory Boy import data_collection.models as dcm ugv_type = dcm . EntityType . objects . create ( name = \"ugv\" , description = \"Unmanned ground vehicle t ype\" , display_name = \"UGV\" , ) ugv_type . capabilities . set ([ radar_cap ]) from data_collection.factories import factories ugv_type = factories . EntityTypeFactory ( name = \"ugv\" , description = \"Unmanned ground vehicle type\" , display_name = \"UGV\" , capabilities = [ radar_cap ], ) Then we can make an individual entity that contains the comms_mod mod. Django ORM Factory Boy import data_collection.models as dcm alpha_1 = dcm . Entity . objects . create ( name = \"alpha_1\" , description = \"A motorized ground rover with multi-comms\" , entity_type = ugv_type , display_name = \"Alpha 1\" , physical_id = \"raspberry pi 3b+\" , ) alpha_1 . mods . set ([ comms_mod ]) from data_collection.factories import factories alpha_1 = factories . EntityFactory ( name = \"alpha_1\" , description = \"A motorized ground rover with multi-comms\" , entity_type = ugv_type , display_name = \"Alpha 1\" , physical_id = \"raspberry pi 3b+\" , mods = [ comms_mod ], ) EntityGroups \u00b6 We can also cluster entities together if there is some other commonality between them. They could be part of a specific scenario, which we could then relate directly on the scenario. They might share a physical property that is not otherwise recorded in Mole or used to denote valid entities for a entity state. Alternatively, EntityGroups can be used to 'tag' entities with an attribute. Django ORM Factory Boy import data_collection.models as dcm rpi_group = dcm . EntityGroup . objects . create ( name = \"rpi\" , description = \"Entities based on a Raspberry Pi platform\" , basemap_element = False , ) alpha_1 . groups . set ([ rpi_group ]) from data_collection.factories import factories rpi_group = factories . EntityGroupFactory ( name = \"rpi\" , description = \"Entities based on a Raspberry Pi platform\" , basemap_element = False , ) alpha_1 . groups . set ([ rpi_group ]) basemap_element indicates whether or not the entities in the given group should be drawn on the front end map or not. Regions \u00b6 Regions represent geographical areas of interest. They can represent a keep-out zone, an activation area, an entity's area of control, etc. Regions are named and are defined by their region type, the geometry of the area it represents, a z-value layer, and an optional geographical point. Scenarios can also optionally be related with specific regions. This could happen if different scenarios affect different test areas or if an activation area changes between rounds. However this relation would need to be specified on the scenario creation rather than the region creation. We'll create a region type first. Django ORM Factory Boy import data_collection.models as dcm koz_type = dcm . RegionType . objects . create ( name = \"keep_out_zone\" , description = \"A keep-out zone\" , ) from data_collection.factories import factories koz_type = factories . RegionTypeFactory ( name = \"keep_out_zone\" , description = \"A keep-out zone\" , ) Now we'll create the region. Django ORM Factory Boy import json import data_collection.models as dcm geometry = { \"coordinates\" : [ [ [ - 117.2443986 , 32.6648668 ], [ - 117.2354937 , 32.6667635 ], [ - 117.2352576 , 32.6721824 ], [ - 117.2471023 , 32.6707012 ], [ - 117.2443986 , 32.6648668 ] ] ], \"type\" : \"Polygon\" } dcm . Region . objects . create ( name = \"keep_out_zone_command_center\" , region_type = koz_type , geom = json . dumps ( geometry ), z_layer = 1.0 , ) from data_collection.factories import factories factories . RegionFactory ( name = \"keep_out_zone_command_center\" , region_type = koz_type , geom = \"POLYGON ((-117.2443986 32.6648668, -117.2354937 32.6667635, -117.2354722 32.6697800, -117.2465873 32.6684434, -117.2443986 32.6648668))\" , z_layer = 1.0 , key_point = None , ) geom can take either a GeoJSON string (as seen in the Django ORM example) or WKT (as seen in the factory_boy example). z_layer can be an arbitrary float value that makes sense in the given context. Note using factory_boy for Region If key_point is not explicitly set to None , the factory will create an arbitrary point for it. PoseSources \u00b6 Mole also has the ability to track entities as they move. We do this through the Pose model. Poses have a PoseSource , which distinguishes poses of different origins. For creating poses later, we'll need to have at least one PoseSource . Django ORM Factory Boy import data_collection.models as dcm gps_pose_source = dcm . PoseSource . objects . create ( name = \"GPS\" , description = \"GPS provided pose source\" , ) from data_collection.factories import factories gps_pose_source = factories . PoseSourceFactory ( name = \"GPS\" , description = \"GPS provided pose source\" , ) Servers \u00b6 In order to display maps, we'll need to set up a Server . This is a model that captures parameters used for the map display. We'll start with a ServerType . Django ORM Factory Boy import data_collection.models as dcm server_type = dcm . ServerType . objects . create ( name = \"Tiled Aerial Imagery Server\" , description = \"Server that provides tiled aerial imagery\" , key = \"tiled_imagery\" , ) from data_collection.factories import factories server_type = factories . ServerTypeFactory ( name = \"Tiled Aerial Imagery Server\" , description = \"Server that provides tiled aerial imagery\" , key = \"tiled_imagery\" , ) Then we'll create some ServerParams . Django ORM Factory Boy import json import data_collection.models as dcm value = { \"lat\" : \"32.709\" , \"lng\" : \"-117.248\" , } map_center_param = dcm . ServerParam . objects . create ( name = \"Map Center\" , description = \"provides parameters for a map's center point\" , param = \"mapOptions\" , value = json . dumps ( value ), ) zoom_param = dcm . ServerParam . objects . create ( name = \"Zoom levels\" , description = \"provides parameters for a map's zoom levels\" , param = \"mapOptions\" , value = '{\"minZoom\":1, \"maxZoom\":20}' , ) import json from data_collection.factories import factories value = { \"lat\" : \"32.709\" , \"lng\" : \"-117.248\" , } map_center_param = factories . ServerParamFactory ( name = \"Map Center\" , description = \"provides parameters for a map's center point\" , param = \"mapOptions\" , value = json . dumps ( value ), ) zoom_param = factories . ServerParamFactory ( name = \"Zoom levels\" , description = \"provides parameters for a map's zoom levels\" , param = \"mapOptions\" , value = '{\"minZoom\":1, \"maxZoom\":20}' , ) Now we can put it together into a Server . For using OpenStreetMap tiles, you can set the base_url to https://a.tile.openstreetmap.org/{z}/{x}/{y}.png . If using the OpenStreetMap tiles, keep the Tile Usage Policy in mind. Django ORM Factory Boy import data_collection.models as dcm server = dcm . Server . objects . create ( name = \"Local World Tiles\" , server_type = server_type \", base_url = \"http:// {window.location.hostname} /maps/styles/ne_simple_style/ {z} / {x} / {y} .png\" , ) server . server_params . set ([ map_center_param , zoom_param ]) from data_collection.factories import factories server = factories . ServerFactory ( name = \"Local World Tiles\" , server_type = server_type \", base_url = \"http:// {window.location.hostname} /maps/styles/ne_simple_style/ {z} / {x} / {y} .png\" , server_params = [ map_center_param , zoom_param ], ) base_url contains the url for the map tiles, whether that's locally served (as in our example) or remotely hosted. Further Configuration \u00b6 Game Clock \u00b6 See Game Clock . Scripted Events \u00b6 See Scenario Scripts . Point Styles \u00b6 See Map/Timeline Marker Styles Entity States \u00b6 See Entity States Access Logs \u00b6 An access log for django can be found at mole/_logs/access.log . This will contain the remote address, username, date of request, where the request is for, HTTP status, response length, and referer.","title":"Basic Configuration"},{"location":"basic_config.html#configuration","text":"","title":"Configuration"},{"location":"basic_config.html#first-steps","text":"If we're starting with a blank slate, we have a couple initial steps to go through to configure Django. We need to create a series of Django model instances to tailor the Mole instrumentation to a specific domain. There are two ways to create instances: using the Django ORM or using third-party library factory_boy . The main difference between the two is how much data must be specified. The ORM requires all the fields to be filled out but factory_boy is configured to fill in fields that are left blank. Regardless of which is chosen, this should be implemented in a custom Django command. An example of one can be found at mole/data_collection/management/commands/configure_mole.py .","title":"First Steps"},{"location":"basic_config.html#locations","text":"The first thing we need to create is a location. Django ORM Factory Boy import data_collection.models as dcm location_1 = dcm . Location . objects . create ( name = \"Location 1\" , description = \"description of location 1\" , point = \"POINT(-117.248 32.709)\" , timezone = \"America/Los_Angeles\" , ) from data_collection.factories import factories location_1 = factories . LocationFactory ( name = \"Location 1\" , description = \"description of location 1\" , point = \"POINT(-117.248 32.709)\" , timezone = \"America/Los_Angeles\" , ) The name and description fields are self-explanatory. The point field is a well-known text representation (WKT) of a point geometry. The timezone field is a string indicating the TZ database name.","title":"Locations"},{"location":"basic_config.html#campaigns","text":"Next we need to create a campaign. A campaign can represent a high level field experiment or test event. Django ORM Factory Boy import data_collection.models as dcm campaign_1 = dcm . Campaign . objects . create ( name = \"Test Event 1\" , description = \"info about test event 1\" , start_datetime = \"2018-10-21T05:00:00-0800\" , end_datetime = \"2018-10-25T19:00:00-0800\" , location = location_1 , trial_id_major_name = \"Day\" , trial_id_minor_name = \"Shift\" , trial_id_micro_name = \"Attempt\" , ) from data_collection.factories import factories campaign_1 = factories . CampaignFactory ( name = \"Test Event 1\" , description = \"info about test event 1\" , start_datetime = \"2018-10-21T05:00:00-0800\" , end_datetime = \"2018-10-25T19:00:00-0800\" , location = location_1 , trial_id_major_name = \"Day\" , trial_id_minor_name = \"Shift\" , trial_id_micro_name = \"Attempt\" , ) start_datetime and end_datetime indicate the start and end time respectively. location should be a reference to the Location object created earlier. trial_id_major_name , trial_id_minor_name , and trial_id_micro_name are strings that describe the numbering scheme for trials. Each trial has a major, minor, and micro id, visualized as x.y.z . For example, the following code block marks the x id as the day. These strings are optional and will default to the empty string if not specified.","title":"Campaigns"},{"location":"basic_config.html#scenarios","text":"Next we'll create a Scenario. But first we need to create a Test Method. A Test Method represents a defined procedure to test a capability. It could be focused on a low level capability such as obstacle detection or path planning, or a system-level test that is intended to exercise the fully-integrated system. A Scenario is a specific instantiation of a Test Method that defines parameters that may be variable in the Test Method (e.g., location, number of agents, test duration, dynamic elements, etc.). So the Test Method might be \"Persistent Swarm Surveillance\", and the Scenario might be \" 3-agent surveillance\". This is intended to allow association of similar Test Methods while allowing some variation in location or parameters. Django ORM Factory Boy import data_collection.models as dcm test_method = dcm . TestMethod . objects . create ( name = \"Interactive Swarm Exercise\" , description = \"Testing swarm capabilities\" , version_major = 1 , version_minor = 0 , version_micro = 0 , variant = \"autonomy\" , ) from data_collection.factories import factories test_method = factories . TestMethodFactory ( name = \"Interactive Swarm Exercise\" , description = \"Testing swarm capabilities\" , version_major = 1 , version_minor = 0 , version_micro = 0 , variant = \"autonomy\" , ) The version numbers and variant can be used to track changes to test procedures. Now we can create the scenario. Django ORM Factory Boy import data_collection.models as dcm scenario_1 = dcm . Scenario . objects . create ( name = \"Scenario 1\" , description = \"description of scenario 1\" , location = location_1 , test_method = test_method , ) from data_collection.factories import factories scenario_1 = factories . ScenarioFactory ( name = \"Scenario 1\" , description = \"description of scenario 1\" , location = location_1 , test_method = test_method , ) The scenario's location could be the same or different than the campaign's location. If different, follow the steps for creating a location to create another one.","title":"Scenarios"},{"location":"basic_config.html#trials","text":"Now we move on to our trials. A Trial can represent a single run or attempt. Events of interest or data about the run can be saved in these trials, giving us an easy container to compare and contrast test results. Since trials contain a lot of data about a run, we'll need to create a couple of pre-requisite instances. The first is a Tester which needs both a user and a role. We use the Django authentication user so unlike the other models, we have to use the factory_boy implementation to ensure that the password is saved correctly. Factory Boy from data_collection.factories import factories main_user = factories . UserFactory ( username = \"admin\" , password = \"admin\" , ) Warning You'll want to adjust this to have more secure credentials. Next we'll create the role. Django ORM Factory Boy import data_collection.models as dcm my_role = dcm . Role . objects . create ( name = \"test_administrator\" , description = \"description of test administrator\" , ) from data_collection.factories import factories my_role = factories . RoleFactory ( name = \"test_administrator\" , description = \"description of test administrator\" , ) Now that we have both a user and a role, we can create a tester. Django ORM Factory Boy import data_collection.models as dcm my_tester = dcm . Tester . objects . create ( user = main_user , role = my_role , ) from data_collection.factories import factories my_tester = factories . TesterFactory ( user = main_user , role = my_role , ) Next we need a system configuration. A system configuration can consist of one or more capability under test, each of which has a performer. We'll start at the bottom with the performer. Django ORM Factory Boy import data_collection.models as dcm performer_1 = dcm . Performer . objects . create ( name = \"Integrator Team 1\" , description = \"info about team 1\" , ) from data_collection.factories import factories performer_1 = factories . PerformerFactory ( name = \"Integrator Team 1\" , description = \"info about team 1\" , ) Next up is capability under test. Django ORM Factory Boy import data_collection.models as dcm capability_under_test = dcm . CapabilityUnderTest . objects . create ( name = \"Swarm Capability\" , description = \"Swarm Capability\" , performer = performer_1 , ) from data_collection.factories import factories capability_under_test = factories . CapabilityUnderTestFactory ( name = \"Swarm Capability\" , description = \"Swarm Capability\" , performer = performer_1 , ) Then we round it out with a system configuration. Django ORM Factory Boy import data_collection.models as dcm sys_conf = dcm . SystemConfiguration . objects . create ( name = \"System Configuration 1\" , description = \"total set of performers\" , capabilities_under_test = ( capability_under_test ,), ) from data_collection.factories import factories sys_conf = factories . SystemConfigurationFactory ( name = \"System Configuration 1\" , description = \"total set of performers\" , capabilities_under_test = ( capability_under_test ,), ) Next we have to create a test condition, as well as a weather instance. First we'll start with the weather. Django ORM Factory Boy import data_collection.models as dcm current_weather = dcm . Weather . objects . create ( name = \"Sunny\" , description = \"no clouds in the sky\" , ) from data_collection.factories import factories current_weather = factories . WeatherFactory ( name = \"Sunny\" , description = \"no clouds in the sky\" , ) Then onto the test condition. Django ORM Factory Boy import data_collection.models as dcm my_test_condition = dcm . TestCondition . objects . create ( weather = current_weather , ) from data_collection.factories import factories my_test_condition = factories . TestConditionFactory ( weather = current_weather , ) Now we can finally create an initial trial. Django ORM Factory Boy import data_collection.models as dcm dcm . Trial . objects . create ( id_major = 0 , id_minor = 0 , id_micro = 0 , campaign = campaign_1 , scenario = scenario_1 , testers = ( my_tester ,), test_condition = my_test_condition , system_configuration = sys_conf , start_datetime = \"2018-10-22T07:00:00-0800\" , reported = False , ) from data_collection.factories import factories factories . TrialFactory ( id_major = 0 , id_minor = 0 , id_micro = 0 , campaign = campaign_1 , scenario = scenario_1 , testers = ( my_tester ,), test_condition = my_test_condition , system_configuration = sys_conf , start_datetime = \"2018-10-22T07:00:00-0800\" , reported = False , ) The ternary id ( id_major , id_minor , id_micro taken together) have to be unique within each campaign, but there is no restriction otherwise on their integer values. reported is a boolean flag that will tell the report generator whether or not to create graphs and charts for this trial.","title":"Trials"},{"location":"basic_config.html#event-types","text":"With a trial in place, we can start creating event types to track the things we are interested in. These event types can represent 'bookmarks' during the test run or a change in state. It can be used to show an interaction of interest or a noteworthy exchange. We'll first create an event level. Django ORM Factory Boy import data_collection.models as dcm event_level = dcm . EventLevel . objects . create ( name = \"Info\" , description = \"Informational events\" , key = \"info\" , visibility = 1 , ) from data_collection.factories import factories event_level = factories . EventLevelFactory ( name = \"Info\" , description = \"Informational events\" , key = \"info\" , visibility = 1 , ) The name and key fields are both unique strings and the visibility is set to some integer value, used to dictate a hierarchy of event types. Now we move on to the event type itself. Django ORM Factory Boy import data_collection.models as dcm dcm . EventType . objects . create ( name = \"Interaction\" , description = \"interaction of note\" , event_level = event_level , is_manual = True , has_duration = False , ) from data_collection.factories import factories factories . EventTypeFactory ( name = \"Interaction\" , description = \"interaction of note\" , event_level = event_level , is_manual = True , has_duration = False , )","title":"Event Types"},{"location":"basic_config.html#entities","text":"This model provides an abstraction for any unit or system we wish to track. This could be a system that's undergoing testing or an orchestration element that we're configuring. Each entity will need an entity type. We can also assign a list of capabilities to an entity type, signifying that every entity of this type has these properties. We can further distinguish individual entities by assigning mods to them, where mods can have multiple capabilities. We'll start with defining a couple of capabilities. Django ORM Factory Boy import data_collection.models as dcm radar_cap = dcm . Capability . objects . create ( name = \"radar\" , description = \"24 GHz Radar\" , display_name = \"Radar\" , ) ble_cap = dcm . Capability . objects . create ( name = \"ble\" , description = \"Capability using Bluetooth Low Energy\" , display_name = \"Bluetooth Low Energy\" , ) from data_collection.factories import factories radar_cap = factories . CapabilityFactory ( name = \"radar\" , description = \"Radar Capability\" , display_name = \"Radar\" , ) ble_cap = factories . CapabilityFactory ( name = \"ble\" , description = \"Capability using Bluetooth Low Energy\" , display_name = \"Bluetooth Low Energy\" , ) display_name is a more human-friendly string that can be used for a UI for example. Next we'll make a mod combining the two capabilities. Django ORM Factory Boy import data_collection.models as dcm comms_mod = dcm . Mod . objects . create ( name = \"comms_mod\" , description = \"A combination mod that contains both radar and bluetooth low energy\" , display_name = \"Combined communication mod\" , ) comms_mod . capabilities . set ([ radar_cap , ble_cap ]) from data_collection.factories import factories comms_mod = factories . ModFactory ( name = \"comms_mod\" , description = \"A combination mod that contains both radar and bluetooth low energy\" , display_name = \"Combined communication mod\" , capabilities = [ radar_cap , ble_cap ], ) Note that when using the Django ORM, the process to add a capability to a mod is slightly more complicated than the creation of the mod. It must be set outside of the creation of the mod due to technical limitations. This occurs in various other models throughout Mole as well. This limitation does not occur in the factory_boy factories. We'll make an entity type next, a ugv entity type with an inherent radar capability. Django ORM Factory Boy import data_collection.models as dcm ugv_type = dcm . EntityType . objects . create ( name = \"ugv\" , description = \"Unmanned ground vehicle t ype\" , display_name = \"UGV\" , ) ugv_type . capabilities . set ([ radar_cap ]) from data_collection.factories import factories ugv_type = factories . EntityTypeFactory ( name = \"ugv\" , description = \"Unmanned ground vehicle type\" , display_name = \"UGV\" , capabilities = [ radar_cap ], ) Then we can make an individual entity that contains the comms_mod mod. Django ORM Factory Boy import data_collection.models as dcm alpha_1 = dcm . Entity . objects . create ( name = \"alpha_1\" , description = \"A motorized ground rover with multi-comms\" , entity_type = ugv_type , display_name = \"Alpha 1\" , physical_id = \"raspberry pi 3b+\" , ) alpha_1 . mods . set ([ comms_mod ]) from data_collection.factories import factories alpha_1 = factories . EntityFactory ( name = \"alpha_1\" , description = \"A motorized ground rover with multi-comms\" , entity_type = ugv_type , display_name = \"Alpha 1\" , physical_id = \"raspberry pi 3b+\" , mods = [ comms_mod ], )","title":"Entities"},{"location":"basic_config.html#entitygroups","text":"We can also cluster entities together if there is some other commonality between them. They could be part of a specific scenario, which we could then relate directly on the scenario. They might share a physical property that is not otherwise recorded in Mole or used to denote valid entities for a entity state. Alternatively, EntityGroups can be used to 'tag' entities with an attribute. Django ORM Factory Boy import data_collection.models as dcm rpi_group = dcm . EntityGroup . objects . create ( name = \"rpi\" , description = \"Entities based on a Raspberry Pi platform\" , basemap_element = False , ) alpha_1 . groups . set ([ rpi_group ]) from data_collection.factories import factories rpi_group = factories . EntityGroupFactory ( name = \"rpi\" , description = \"Entities based on a Raspberry Pi platform\" , basemap_element = False , ) alpha_1 . groups . set ([ rpi_group ]) basemap_element indicates whether or not the entities in the given group should be drawn on the front end map or not.","title":"EntityGroups"},{"location":"basic_config.html#regions","text":"Regions represent geographical areas of interest. They can represent a keep-out zone, an activation area, an entity's area of control, etc. Regions are named and are defined by their region type, the geometry of the area it represents, a z-value layer, and an optional geographical point. Scenarios can also optionally be related with specific regions. This could happen if different scenarios affect different test areas or if an activation area changes between rounds. However this relation would need to be specified on the scenario creation rather than the region creation. We'll create a region type first. Django ORM Factory Boy import data_collection.models as dcm koz_type = dcm . RegionType . objects . create ( name = \"keep_out_zone\" , description = \"A keep-out zone\" , ) from data_collection.factories import factories koz_type = factories . RegionTypeFactory ( name = \"keep_out_zone\" , description = \"A keep-out zone\" , ) Now we'll create the region. Django ORM Factory Boy import json import data_collection.models as dcm geometry = { \"coordinates\" : [ [ [ - 117.2443986 , 32.6648668 ], [ - 117.2354937 , 32.6667635 ], [ - 117.2352576 , 32.6721824 ], [ - 117.2471023 , 32.6707012 ], [ - 117.2443986 , 32.6648668 ] ] ], \"type\" : \"Polygon\" } dcm . Region . objects . create ( name = \"keep_out_zone_command_center\" , region_type = koz_type , geom = json . dumps ( geometry ), z_layer = 1.0 , ) from data_collection.factories import factories factories . RegionFactory ( name = \"keep_out_zone_command_center\" , region_type = koz_type , geom = \"POLYGON ((-117.2443986 32.6648668, -117.2354937 32.6667635, -117.2354722 32.6697800, -117.2465873 32.6684434, -117.2443986 32.6648668))\" , z_layer = 1.0 , key_point = None , ) geom can take either a GeoJSON string (as seen in the Django ORM example) or WKT (as seen in the factory_boy example). z_layer can be an arbitrary float value that makes sense in the given context. Note using factory_boy for Region If key_point is not explicitly set to None , the factory will create an arbitrary point for it.","title":"Regions"},{"location":"basic_config.html#posesources","text":"Mole also has the ability to track entities as they move. We do this through the Pose model. Poses have a PoseSource , which distinguishes poses of different origins. For creating poses later, we'll need to have at least one PoseSource . Django ORM Factory Boy import data_collection.models as dcm gps_pose_source = dcm . PoseSource . objects . create ( name = \"GPS\" , description = \"GPS provided pose source\" , ) from data_collection.factories import factories gps_pose_source = factories . PoseSourceFactory ( name = \"GPS\" , description = \"GPS provided pose source\" , )","title":"PoseSources"},{"location":"basic_config.html#servers","text":"In order to display maps, we'll need to set up a Server . This is a model that captures parameters used for the map display. We'll start with a ServerType . Django ORM Factory Boy import data_collection.models as dcm server_type = dcm . ServerType . objects . create ( name = \"Tiled Aerial Imagery Server\" , description = \"Server that provides tiled aerial imagery\" , key = \"tiled_imagery\" , ) from data_collection.factories import factories server_type = factories . ServerTypeFactory ( name = \"Tiled Aerial Imagery Server\" , description = \"Server that provides tiled aerial imagery\" , key = \"tiled_imagery\" , ) Then we'll create some ServerParams . Django ORM Factory Boy import json import data_collection.models as dcm value = { \"lat\" : \"32.709\" , \"lng\" : \"-117.248\" , } map_center_param = dcm . ServerParam . objects . create ( name = \"Map Center\" , description = \"provides parameters for a map's center point\" , param = \"mapOptions\" , value = json . dumps ( value ), ) zoom_param = dcm . ServerParam . objects . create ( name = \"Zoom levels\" , description = \"provides parameters for a map's zoom levels\" , param = \"mapOptions\" , value = '{\"minZoom\":1, \"maxZoom\":20}' , ) import json from data_collection.factories import factories value = { \"lat\" : \"32.709\" , \"lng\" : \"-117.248\" , } map_center_param = factories . ServerParamFactory ( name = \"Map Center\" , description = \"provides parameters for a map's center point\" , param = \"mapOptions\" , value = json . dumps ( value ), ) zoom_param = factories . ServerParamFactory ( name = \"Zoom levels\" , description = \"provides parameters for a map's zoom levels\" , param = \"mapOptions\" , value = '{\"minZoom\":1, \"maxZoom\":20}' , ) Now we can put it together into a Server . For using OpenStreetMap tiles, you can set the base_url to https://a.tile.openstreetmap.org/{z}/{x}/{y}.png . If using the OpenStreetMap tiles, keep the Tile Usage Policy in mind. Django ORM Factory Boy import data_collection.models as dcm server = dcm . Server . objects . create ( name = \"Local World Tiles\" , server_type = server_type \", base_url = \"http:// {window.location.hostname} /maps/styles/ne_simple_style/ {z} / {x} / {y} .png\" , ) server . server_params . set ([ map_center_param , zoom_param ]) from data_collection.factories import factories server = factories . ServerFactory ( name = \"Local World Tiles\" , server_type = server_type \", base_url = \"http:// {window.location.hostname} /maps/styles/ne_simple_style/ {z} / {x} / {y} .png\" , server_params = [ map_center_param , zoom_param ], ) base_url contains the url for the map tiles, whether that's locally served (as in our example) or remotely hosted.","title":"Servers"},{"location":"basic_config.html#further-configuration","text":"","title":"Further Configuration"},{"location":"basic_config.html#game-clock","text":"See Game Clock .","title":"Game Clock"},{"location":"basic_config.html#scripted-events","text":"See Scenario Scripts .","title":"Scripted Events"},{"location":"basic_config.html#point-styles","text":"See Map/Timeline Marker Styles","title":"Point Styles"},{"location":"basic_config.html#entity-states","text":"See Entity States","title":"Entity States"},{"location":"basic_config.html#access-logs","text":"An access log for django can be found at mole/_logs/access.log . This will contain the remote address, username, date of request, where the request is for, HTTP status, response length, and referer.","title":"Access Logs"},{"location":"developer_angular.html","text":"User Interface \u00b6 Introduction \u00b6 The Mole user interface is built using the Angular web framework. You can read more about the Angular framework here . Getting Started \u00b6 Install Angular CLI \u00b6 Before beginning development, it's recommended that you insall the Angular CLI. The CLI will give you a library of commands to generate components, services, modules, etc. as well as check for updates and run unit tests. Install: npm install -g @angular/cli Code Styling \u00b6 Variables : camelCase Service instances prefixed with underscore Indentation : 2 spaces Curly Brackets : Openning bracket inline with header Project Structure \u00b6 The root of the project's working files is angular > app > src > app . The folder structure for the project was heavily informed by this article . Modules \u00b6 Modules can be thought of as the pages within Mole. They are composed of a module typescript file and a component (generated seperately, see Components . Modules define the placement and layout of the components that make up a page. When generating a module, the Angular CLI will create a single typescript file where you can import the resources you would like to expose to components within the module. Generate a module within cwd: Local: ng g m {module_name} Shared: ng g m --module shared {module_name} Components \u00b6 The user interface is broken into UI building blocks called \"components\". Every component has its own html, typescript, sass, and unit test file within a containing directory. When generating a component, the Angular CLI will create a named directory with template component files. Every component provides a custom html tag to be used throughout the app. Generate a component within cwd: Local: ng g c {component_name} Shared: ng g c --module shared {component_name} Important The component typescript file should be restricted to only contain view logic. Logic outside of this scope, especially if it will be used by other components, should be contained within a service . Services \u00b6 Services perform tasks for components and are especially useful when components are sharing the same resources. When generating a service, the Angular CLI will create a typescript file and unit test file within a named directory. Generate a local service within cwd: Local: ng g s {service_name} Shared: ng g s --module shared {service_name} Models \u00b6 Models are simply interfaces. All models are stored within angular > app > src > app > shared > models . You may allow multiple related models to share the same model file. If your model is an interface for an api model, you will need to write an adapter for the model. An adapter is essentially a function that taks a json object as a paramter and returns an instance of the model (see exmple below). Generate a model: Create a new typescript file following this naming scheme: {model_name}.model.ts Update index.ts to export the models you created eventTypeAdapter ( json : any ) : EventType { let eventType : EventType = { url : \"\" , id : - 1 , name : json.name , description : json.description , eventLevel : json.event_level , ... }; if ( json . url ) { eventType . url = json . url . replace ( \"http://django:8000\" , \"\" ); } if ( json . id ) { eventType . id = json . id ; } return eventType ; } Shared Directory \u00b6 Most components can be shared among multiple modules, that's the purpose of the angular > app > src > app > shared directory: to expose common components to all the modules. If you find yourself developing a component or service that will only be used by a single module, this code may be placed within that module's directory. Routing \u00b6 All app routes are defined in app-routing.module.ts . This file contains two exported constants: routes and moleLinks . routes contains all the route objects for internal modules and are lazy loaded moleLinks contains all the route objects for external links When you add a route object to this file, the side navigation menu is configured to list it without any additional configuration. Sharing Data Between Components \u00b6 There are a few ways of sharing data between components, and Mole utilizes all of them. When developing the UI, it's important to be familiar with the @Input, @Output, and @ViewChild decorators, as well as rxjs Observables. A good resource for these concepts can bew viewed here . Shared Modules \u00b6 The shared directly also includes two shared modules: molemat.module.ts and shared.module.ts . molemat.module.ts : This module imports and exports all Angular Material modules so they can be used throughout the application. shared.module.ts : This module imports all modules that are not Angular Material modules and exports all shared components. UI Styling \u00b6 Colors \u00b6 All UI colors are defined in an Angular Material theme located in angular > src > styles > _theme.scss . Read more about theming Angular Material here . Important It is recommended not to hard-code colors within components and to reference the theme instead. Iconography \u00b6 There are two different icon libraries available for use throughout the project: Material Design Icons See usage docs here . See available icons here . Font Awesome See usage docs here . See available icons here . Point Styles \u00b6 Point styles define the colors and icons of map markers and event types. Read more about point styles here . Unit Testing \u00b6 Refer to Angular Testing for more details on unit testing. Run unit tests: From angular > app > src > app , run ng test Development Server \u00b6 The Angular development server hosts Mole on port 4200 . The Angular development server automatically spins up when you start mole. As changes are made in the codebase, the application auto-reloads to reflect the changes. This allows for a more rapid development experience. Serve Static Files from Django \u00b6 The Angular container is able to build the Angular files to be served statically from Django. To build the files, use the -a flag on the init script. Mole will spin up a one-time angular container to build the static files into the docker volume. Once complete, the angular container will delete itself. Static files are hosted on port 8000/a . Build Angular files to be served from Django: ./ml init -a","title":"User Interface"},{"location":"developer_angular.html#user-interface","text":"","title":"User Interface"},{"location":"developer_angular.html#introduction","text":"The Mole user interface is built using the Angular web framework. You can read more about the Angular framework here .","title":"Introduction"},{"location":"developer_angular.html#getting-started","text":"","title":"Getting Started"},{"location":"developer_angular.html#install-angular-cli","text":"Before beginning development, it's recommended that you insall the Angular CLI. The CLI will give you a library of commands to generate components, services, modules, etc. as well as check for updates and run unit tests. Install: npm install -g @angular/cli","title":"Install Angular CLI"},{"location":"developer_angular.html#code-styling","text":"Variables : camelCase Service instances prefixed with underscore Indentation : 2 spaces Curly Brackets : Openning bracket inline with header","title":"Code Styling"},{"location":"developer_angular.html#project-structure","text":"The root of the project's working files is angular > app > src > app . The folder structure for the project was heavily informed by this article .","title":"Project Structure"},{"location":"developer_angular.html#modules","text":"Modules can be thought of as the pages within Mole. They are composed of a module typescript file and a component (generated seperately, see Components . Modules define the placement and layout of the components that make up a page. When generating a module, the Angular CLI will create a single typescript file where you can import the resources you would like to expose to components within the module. Generate a module within cwd: Local: ng g m {module_name} Shared: ng g m --module shared {module_name}","title":"Modules"},{"location":"developer_angular.html#components","text":"The user interface is broken into UI building blocks called \"components\". Every component has its own html, typescript, sass, and unit test file within a containing directory. When generating a component, the Angular CLI will create a named directory with template component files. Every component provides a custom html tag to be used throughout the app. Generate a component within cwd: Local: ng g c {component_name} Shared: ng g c --module shared {component_name} Important The component typescript file should be restricted to only contain view logic. Logic outside of this scope, especially if it will be used by other components, should be contained within a service .","title":"Components"},{"location":"developer_angular.html#services","text":"Services perform tasks for components and are especially useful when components are sharing the same resources. When generating a service, the Angular CLI will create a typescript file and unit test file within a named directory. Generate a local service within cwd: Local: ng g s {service_name} Shared: ng g s --module shared {service_name}","title":"Services"},{"location":"developer_angular.html#models","text":"Models are simply interfaces. All models are stored within angular > app > src > app > shared > models . You may allow multiple related models to share the same model file. If your model is an interface for an api model, you will need to write an adapter for the model. An adapter is essentially a function that taks a json object as a paramter and returns an instance of the model (see exmple below). Generate a model: Create a new typescript file following this naming scheme: {model_name}.model.ts Update index.ts to export the models you created eventTypeAdapter ( json : any ) : EventType { let eventType : EventType = { url : \"\" , id : - 1 , name : json.name , description : json.description , eventLevel : json.event_level , ... }; if ( json . url ) { eventType . url = json . url . replace ( \"http://django:8000\" , \"\" ); } if ( json . id ) { eventType . id = json . id ; } return eventType ; }","title":"Models"},{"location":"developer_angular.html#shared-directory","text":"Most components can be shared among multiple modules, that's the purpose of the angular > app > src > app > shared directory: to expose common components to all the modules. If you find yourself developing a component or service that will only be used by a single module, this code may be placed within that module's directory.","title":"Shared Directory"},{"location":"developer_angular.html#routing","text":"All app routes are defined in app-routing.module.ts . This file contains two exported constants: routes and moleLinks . routes contains all the route objects for internal modules and are lazy loaded moleLinks contains all the route objects for external links When you add a route object to this file, the side navigation menu is configured to list it without any additional configuration.","title":"Routing"},{"location":"developer_angular.html#sharing-data-between-components","text":"There are a few ways of sharing data between components, and Mole utilizes all of them. When developing the UI, it's important to be familiar with the @Input, @Output, and @ViewChild decorators, as well as rxjs Observables. A good resource for these concepts can bew viewed here .","title":"Sharing Data Between Components"},{"location":"developer_angular.html#shared-modules","text":"The shared directly also includes two shared modules: molemat.module.ts and shared.module.ts . molemat.module.ts : This module imports and exports all Angular Material modules so they can be used throughout the application. shared.module.ts : This module imports all modules that are not Angular Material modules and exports all shared components.","title":"Shared Modules"},{"location":"developer_angular.html#ui-styling","text":"","title":"UI Styling"},{"location":"developer_angular.html#colors","text":"All UI colors are defined in an Angular Material theme located in angular > src > styles > _theme.scss . Read more about theming Angular Material here . Important It is recommended not to hard-code colors within components and to reference the theme instead.","title":"Colors"},{"location":"developer_angular.html#iconography","text":"There are two different icon libraries available for use throughout the project: Material Design Icons See usage docs here . See available icons here . Font Awesome See usage docs here . See available icons here .","title":"Iconography"},{"location":"developer_angular.html#point-styles","text":"Point styles define the colors and icons of map markers and event types. Read more about point styles here .","title":"Point Styles"},{"location":"developer_angular.html#unit-testing","text":"Refer to Angular Testing for more details on unit testing. Run unit tests: From angular > app > src > app , run ng test","title":"Unit Testing"},{"location":"developer_angular.html#development-server","text":"The Angular development server hosts Mole on port 4200 . The Angular development server automatically spins up when you start mole. As changes are made in the codebase, the application auto-reloads to reflect the changes. This allows for a more rapid development experience.","title":"Development Server"},{"location":"developer_angular.html#serve-static-files-from-django","text":"The Angular container is able to build the Angular files to be served statically from Django. To build the files, use the -a flag on the init script. Mole will spin up a one-time angular container to build the static files into the docker volume. Once complete, the angular container will delete itself. Static files are hosted on port 8000/a . Build Angular files to be served from Django: ./ml init -a","title":"Serve Static Files from Django"},{"location":"developer_angular_testing.html","text":"ANGULAR MOLE TESTING \u00b6 For Angular Mole we are using the Jasmine Unit Testing Framework . Another Jasmine Intro Running Tests \u00b6 Navigate to the app directory angular/app/src/app Then from terminal use command ng test to run the entire test suite. Basics Of Tests \u00b6 Each component, pipe, service, etc has a test suite that is associated with it. To define a test suite in Jasmine we use the describe function that takes two parameters: - String : The title of the suite - function : Function that implements the suite Each suite is made up of unit tests that are each defined with the it function which also takes two parameters: - String : Name of the unit test - function : Function that implements the unit test Running In Isolation \u00b6 Before Each Function \u00b6 To Run tests in true isolation we set up all construction of (components / objects / services / etc) along with any other setup before we run an it() spec unit test. When beforeEach() is placed within the test suite define() it will automatically be run before each it() within the test suite. beforeEach() contains built in async capabilities which automatically calls done() before moving on to each unit test. After Each Function \u00b6 Similar to the beforeEach() , afterEach() when placed within the test suite define() , will run after each it() unit test. This is especially useful for any kind of cleanup needed or when using mock HttpCalls. TestBed \u00b6 TestBed is the primary API for unit testing Angular applications. TestBed will configure and initialize the environment which grants methods to help in construction of components, services, etc. Allows overriding default providers, directives, pipes, modules of the test injector, which are defined in test_injector.js configureTestingModule \u00b6 When configuring the test environment with imports, providers, and declarations (just like one would use for component creation), the configureTestingModule() is used where a moduleDefinition object is passed to it. **For anything being tested that has dependencies on other services or whatnot, will be mocked to maintain isolation testing. To do this one overrides the component and uses factorys to generate mock objects wherever specified dependency is needed. Ex. ImageDialogComponent relies on MatDialogRef and MAT_DIALOG_DATA to build beforeEach(async(() => { TestBed.configureTestingModule({ declarations: [ ImageDialogComponent ] }).overrideComponent(ImageDialogComponent, { set: { providers: [ {provide: MatDialogRef, useFactory: () => new MockMatDialogRef()}, {provide: MAT_DIALOG_DATA, useFactory: () => {} } ], } }).compileComponents(); })); More On Injecting Mock classes for Testing Testing Template HTML CODE \u00b6 When testing templates you will use the debugElement to access the DOM properties and attributes of the component fixture = TestBed.createComponent(ImageDialogComponent); component = fixture.debugElement.componentInstance; with the debugElement one can query through css to grab any element in the DOM and test various aspects of it such as is it null due to a ngIf directive: let title_heading = fixture.debugElement.query(By.css('h1')); expect(title_heading).not.toBeNull(); does it have certain property values set: expect(title_heading.properties.innerText.trim()).toEqual(\"Images\"); triggers an event tied to it (like a button with click event): let button = fixture.debugElement.query(By.css('button')); button.triggerEventHandler('click', {}); expect(on_event_type_click_spy).toHaveBeenCalled(); ( In this example we use triggerEventHandler which takes two arguments 1: the name of event as string, in this case 'click' 2: the event object that will be passed ) ** We can also test to see if the triggered event is called with a specific value with toHaveBeenCalledWith( args ) ***!!! WHEN TESTING TEMPLATE CODE NO CHANGES WILL HAPPEN IN TEST SUITE TILL CHANGES ARE DETECTED !!! To do this use: detectChanges() So if above button trigger event test were run without detectChanges it will fail since detectChanges was not run after initiating the triggerEventHandler so full working example would be: let button = fixture.debugElement.query(By.css('button')); button.triggerEventHandler('click', {}); fixture.detectChanges(); expect(on_event_type_click_spy).toHaveBeenCalled(); This applies for all changes to template such as an ngIf or ngFor based on specific conditions that were changed since test suite compiled the component to be tested. Spies \u00b6 Spies are a useful testing tool that allows one to take an object and override that objects specific method so that at anytime during the running of the test when a specific method that is being spied on is called, the specified spy function will run instead. This is especially useful when you have a method that needs to call other class methods to complete. Since we want as much isolation as possible when testing we spy on these methods and return mock values to see if the outer function being tested works. Ex. //Spy on open method belonging to the components dialog object and whenever called return 7 let dialog_spy = spyOn(component.dialog, 'open').and.returnValue( 7 ); Spies will exist within the scope they are declared. Any global spies will exists throughout all of the tests while those declared within a unit test will only be available to that specific test. Another purpose of spies is to see if the spied on method is called or not expect(dialog_spy).toHaveBeenCalled(); or if its called with specific arguments expect(dialog_spy).toHaveBeenCalledWith( {'one': 1, 'two': 2} ); If a global spy is set and you need to call the actual implementation of the method use callThrough //This will now call the original method whenever the method being spied on is called dialog_spy.and.callThrough(); !!!! You CANNOT re-declare a spy on a method if the method is already being spied upon, if this is needed just change its value !!!! //change spies value from returning 7 to calling a fake function that returns the string 'hello' let dialog_spy = spyOn(component.dialog, 'open').and.returnValue( 7 ); dialog_spy.and.callFake( () => { return 'hello' } ); !!!!!!!!Allowing re-declaration of spies for methods can be bypassed by setting the global settings to allow it but this is not recommended !!!!!!!! jasmine.getEnv().allowRespy(true); Lifecycle Hook Testing \u00b6 When testing lifecycle hook functions that are part of the component creation process such as ngAfterInit we can mock the component to properly test these methods in isolation. We mock the component by creating a mock class that extends the component to be tested and use spies in the method calls before calling the super.method_name() ngOnChanges \u00b6 When testing ngOnChange methods in a testbed you have to update the @input attributes through a wrapper test component and pass the changes to the component being tested... cant just set the input value and call detect, wont work....has to be updated through the view (basically angular has to do it) For an Example see: event-type-cards.component.spec.ts Consturctor Tests \u00b6 If you need to spy on methods or have certain things set before constructor for whatever is being tested is run, a great spot to do setup is in another beforeEach() block. You can have multiple beforeEach() blocks that will all run before the creation of whatever is being tested. //In this case say we have asyncronous setup then once done set values before creation happens during the first test beforeEach(async(() => { ....Do stuff })); beforeEach(() => { ...Other stuff done, now Do More Stuff }); it('first test', ()=>{ ... });","title":"Angular Testing"},{"location":"developer_angular_testing.html#angular-mole-testing","text":"For Angular Mole we are using the Jasmine Unit Testing Framework . Another Jasmine Intro","title":"ANGULAR MOLE TESTING"},{"location":"developer_angular_testing.html#running-tests","text":"Navigate to the app directory angular/app/src/app Then from terminal use command ng test to run the entire test suite.","title":"Running Tests"},{"location":"developer_angular_testing.html#basics-of-tests","text":"Each component, pipe, service, etc has a test suite that is associated with it. To define a test suite in Jasmine we use the describe function that takes two parameters: - String : The title of the suite - function : Function that implements the suite Each suite is made up of unit tests that are each defined with the it function which also takes two parameters: - String : Name of the unit test - function : Function that implements the unit test","title":"Basics Of Tests"},{"location":"developer_angular_testing.html#running-in-isolation","text":"","title":"Running In Isolation"},{"location":"developer_angular_testing.html#before-each-function","text":"To Run tests in true isolation we set up all construction of (components / objects / services / etc) along with any other setup before we run an it() spec unit test. When beforeEach() is placed within the test suite define() it will automatically be run before each it() within the test suite. beforeEach() contains built in async capabilities which automatically calls done() before moving on to each unit test.","title":"Before Each Function"},{"location":"developer_angular_testing.html#after-each-function","text":"Similar to the beforeEach() , afterEach() when placed within the test suite define() , will run after each it() unit test. This is especially useful for any kind of cleanup needed or when using mock HttpCalls.","title":"After Each Function"},{"location":"developer_angular_testing.html#testbed","text":"TestBed is the primary API for unit testing Angular applications. TestBed will configure and initialize the environment which grants methods to help in construction of components, services, etc. Allows overriding default providers, directives, pipes, modules of the test injector, which are defined in test_injector.js","title":"TestBed"},{"location":"developer_angular_testing.html#configuretestingmodule","text":"When configuring the test environment with imports, providers, and declarations (just like one would use for component creation), the configureTestingModule() is used where a moduleDefinition object is passed to it. **For anything being tested that has dependencies on other services or whatnot, will be mocked to maintain isolation testing. To do this one overrides the component and uses factorys to generate mock objects wherever specified dependency is needed. Ex. ImageDialogComponent relies on MatDialogRef and MAT_DIALOG_DATA to build beforeEach(async(() => { TestBed.configureTestingModule({ declarations: [ ImageDialogComponent ] }).overrideComponent(ImageDialogComponent, { set: { providers: [ {provide: MatDialogRef, useFactory: () => new MockMatDialogRef()}, {provide: MAT_DIALOG_DATA, useFactory: () => {} } ], } }).compileComponents(); })); More On Injecting Mock classes for Testing","title":"configureTestingModule"},{"location":"developer_angular_testing.html#testing-template-html-code","text":"When testing templates you will use the debugElement to access the DOM properties and attributes of the component fixture = TestBed.createComponent(ImageDialogComponent); component = fixture.debugElement.componentInstance; with the debugElement one can query through css to grab any element in the DOM and test various aspects of it such as is it null due to a ngIf directive: let title_heading = fixture.debugElement.query(By.css('h1')); expect(title_heading).not.toBeNull(); does it have certain property values set: expect(title_heading.properties.innerText.trim()).toEqual(\"Images\"); triggers an event tied to it (like a button with click event): let button = fixture.debugElement.query(By.css('button')); button.triggerEventHandler('click', {}); expect(on_event_type_click_spy).toHaveBeenCalled(); ( In this example we use triggerEventHandler which takes two arguments 1: the name of event as string, in this case 'click' 2: the event object that will be passed ) ** We can also test to see if the triggered event is called with a specific value with toHaveBeenCalledWith( args ) ***!!! WHEN TESTING TEMPLATE CODE NO CHANGES WILL HAPPEN IN TEST SUITE TILL CHANGES ARE DETECTED !!! To do this use: detectChanges() So if above button trigger event test were run without detectChanges it will fail since detectChanges was not run after initiating the triggerEventHandler so full working example would be: let button = fixture.debugElement.query(By.css('button')); button.triggerEventHandler('click', {}); fixture.detectChanges(); expect(on_event_type_click_spy).toHaveBeenCalled(); This applies for all changes to template such as an ngIf or ngFor based on specific conditions that were changed since test suite compiled the component to be tested.","title":"Testing Template HTML CODE"},{"location":"developer_angular_testing.html#spies","text":"Spies are a useful testing tool that allows one to take an object and override that objects specific method so that at anytime during the running of the test when a specific method that is being spied on is called, the specified spy function will run instead. This is especially useful when you have a method that needs to call other class methods to complete. Since we want as much isolation as possible when testing we spy on these methods and return mock values to see if the outer function being tested works. Ex. //Spy on open method belonging to the components dialog object and whenever called return 7 let dialog_spy = spyOn(component.dialog, 'open').and.returnValue( 7 ); Spies will exist within the scope they are declared. Any global spies will exists throughout all of the tests while those declared within a unit test will only be available to that specific test. Another purpose of spies is to see if the spied on method is called or not expect(dialog_spy).toHaveBeenCalled(); or if its called with specific arguments expect(dialog_spy).toHaveBeenCalledWith( {'one': 1, 'two': 2} ); If a global spy is set and you need to call the actual implementation of the method use callThrough //This will now call the original method whenever the method being spied on is called dialog_spy.and.callThrough(); !!!! You CANNOT re-declare a spy on a method if the method is already being spied upon, if this is needed just change its value !!!! //change spies value from returning 7 to calling a fake function that returns the string 'hello' let dialog_spy = spyOn(component.dialog, 'open').and.returnValue( 7 ); dialog_spy.and.callFake( () => { return 'hello' } ); !!!!!!!!Allowing re-declaration of spies for methods can be bypassed by setting the global settings to allow it but this is not recommended !!!!!!!! jasmine.getEnv().allowRespy(true);","title":"Spies"},{"location":"developer_angular_testing.html#lifecycle-hook-testing","text":"When testing lifecycle hook functions that are part of the component creation process such as ngAfterInit we can mock the component to properly test these methods in isolation. We mock the component by creating a mock class that extends the component to be tested and use spies in the method calls before calling the super.method_name()","title":"Lifecycle Hook Testing"},{"location":"developer_angular_testing.html#ngonchanges","text":"When testing ngOnChange methods in a testbed you have to update the @input attributes through a wrapper test component and pass the changes to the component being tested... cant just set the input value and call detect, wont work....has to be updated through the view (basically angular has to do it) For an Example see: event-type-cards.component.spec.ts","title":"ngOnChanges"},{"location":"developer_angular_testing.html#consturctor-tests","text":"If you need to spy on methods or have certain things set before constructor for whatever is being tested is run, a great spot to do setup is in another beforeEach() block. You can have multiple beforeEach() blocks that will all run before the creation of whatever is being tested. //In this case say we have asyncronous setup then once done set values before creation happens during the first test beforeEach(async(() => { ....Do stuff })); beforeEach(() => { ...Other stuff done, now Do More Stuff }); it('first test', ()=>{ ... });","title":"Consturctor Tests"},{"location":"developer_debugger.html","text":"Debugger \u00b6 Debugging the Mole API backend can be simplified using an IDE debugger. Mole is distributed with a configuration for debugging under Visual Studio Code. This functionality makes use of the debugpy debugger for remote debugging within a Docker container: https://code.visualstudio.com/docs/containers/debug-python . Visual Studio Code uses the .vscode/launch.json file to configure the remote debugging session. This file is included in the Mole repository, so no added steps should be necessary to use it. In order to run the debugger, the Gunicorn WSGI server must be started with a single process and the debugpy server must be started within it. These are accomplished by passing the -d parameter to either ./ml init or ./ml run (e.g., ./ml run -d ) Note In debug mode, Django will not serve pages until a debugger client has connected. When run with -d , once all containers have started, the Django server waits for a debugger client to \"attach\". In order to attach the Visual Studio Code debugger, click on Run -> Start Debugging and select \"Django: Remote Attach\" or click on the Run/Debug icon in the side panel and click the green play icon ( Start Debugging ) with \"Django: Remote Attach\" selected from the pull-down. Attach to the debugger using the green play icon with \"Django: Remote Attach\" selected from the pull down. When the debugger client has successfully attached to the debugger server within the container, the bar at the bottom of the Visual Studio Code window may turn from blue to orange (depending on the theme in use). Additionally, the Mole terminal output indicates Debugger client attached. Continuing. when the debugger is successfully attached. Visual Studio Code debugger not attached Visual Studio Code debugger attached From here all Visual Studio Code Debugger functionality is available including the following: breakpoints logpoints step through / into / over data inspection watch expressions debug console For an introduction to setting breakpoints and working with the Visual Studio Code debugger, see https://code.visualstudio.com/docs/python/python-tutorial#_configure-and-run-the-debugger . More in depth documentation on working with the Visual Studio Code debugger can be found here: https://code.visualstudio.com/docs/editor/debugging","title":"Debugger"},{"location":"developer_debugger.html#debugger","text":"Debugging the Mole API backend can be simplified using an IDE debugger. Mole is distributed with a configuration for debugging under Visual Studio Code. This functionality makes use of the debugpy debugger for remote debugging within a Docker container: https://code.visualstudio.com/docs/containers/debug-python . Visual Studio Code uses the .vscode/launch.json file to configure the remote debugging session. This file is included in the Mole repository, so no added steps should be necessary to use it. In order to run the debugger, the Gunicorn WSGI server must be started with a single process and the debugpy server must be started within it. These are accomplished by passing the -d parameter to either ./ml init or ./ml run (e.g., ./ml run -d ) Note In debug mode, Django will not serve pages until a debugger client has connected. When run with -d , once all containers have started, the Django server waits for a debugger client to \"attach\". In order to attach the Visual Studio Code debugger, click on Run -> Start Debugging and select \"Django: Remote Attach\" or click on the Run/Debug icon in the side panel and click the green play icon ( Start Debugging ) with \"Django: Remote Attach\" selected from the pull-down. Attach to the debugger using the green play icon with \"Django: Remote Attach\" selected from the pull down. When the debugger client has successfully attached to the debugger server within the container, the bar at the bottom of the Visual Studio Code window may turn from blue to orange (depending on the theme in use). Additionally, the Mole terminal output indicates Debugger client attached. Continuing. when the debugger is successfully attached. Visual Studio Code debugger not attached Visual Studio Code debugger attached From here all Visual Studio Code Debugger functionality is available including the following: breakpoints logpoints step through / into / over data inspection watch expressions debug console For an introduction to setting breakpoints and working with the Visual Studio Code debugger, see https://code.visualstudio.com/docs/python/python-tutorial#_configure-and-run-the-debugger . More in depth documentation on working with the Visual Studio Code debugger can be found here: https://code.visualstudio.com/docs/editor/debugging","title":"Debugger"},{"location":"developer_migrations.html","text":"Migrations \u00b6 There are two primary purposes for migrations in Mole: To track changes made to models.py To bring legacy databases into compliance Track Model Changes \u00b6 As Mole evolves, our database model will be modified and adapted to better serve the purposes of Mole. Django uses migration files to migrate databases into new schemas. Whenever a model is changed and committed, it's important to also commit the correlating migration file. To generate the migration file, run ./ml django -mm while Mole is running. The new migration file will be found in the mole > data_collection > migrations directory. Migrate Legacy Databases \u00b6 You may find yourself in a position where you would like to view data in mole from an old experiment that used a different database schema. In this case you will need to write a custom migration file and run it against your database. Note Some custom migrations have already been developed. If you have an existing custom migration file, you can skip to step 3. Identify the schema differences. If you have the models.py file from the time the database was created, you can look at the models.py diff. If not, you will need to view the database in an application like TablePlus and take note of schema differences there. Write custom migrations. From the differences identified in the schema, follow the Django migrations documentation to write the migration instructions. Apply the migration. Verify the custom migration is in the mole > data_collection > migrations directory and then shell into the Django container. From the Django container, run ./manage.py migrate data_collection [your_custom_migration] Verify all required migrations were made. As a quick check to see if you caught all schema differences in your migration, run ./ml django -mm from the. If all differences were caught, Django will return \"No changes detected\". If not, Django will make a new migration file with the updates that you need to make to your custom migration file. Save a copy of your migrated database. Run ./ml db -b to create a backup of the database in the db_backups > backups directory. The original DB, custom migration, and migrated DB should be saved together.","title":"Migrations"},{"location":"developer_migrations.html#migrations","text":"There are two primary purposes for migrations in Mole: To track changes made to models.py To bring legacy databases into compliance","title":"Migrations"},{"location":"developer_migrations.html#track-model-changes","text":"As Mole evolves, our database model will be modified and adapted to better serve the purposes of Mole. Django uses migration files to migrate databases into new schemas. Whenever a model is changed and committed, it's important to also commit the correlating migration file. To generate the migration file, run ./ml django -mm while Mole is running. The new migration file will be found in the mole > data_collection > migrations directory.","title":"Track Model Changes"},{"location":"developer_migrations.html#migrate-legacy-databases","text":"You may find yourself in a position where you would like to view data in mole from an old experiment that used a different database schema. In this case you will need to write a custom migration file and run it against your database. Note Some custom migrations have already been developed. If you have an existing custom migration file, you can skip to step 3. Identify the schema differences. If you have the models.py file from the time the database was created, you can look at the models.py diff. If not, you will need to view the database in an application like TablePlus and take note of schema differences there. Write custom migrations. From the differences identified in the schema, follow the Django migrations documentation to write the migration instructions. Apply the migration. Verify the custom migration is in the mole > data_collection > migrations directory and then shell into the Django container. From the Django container, run ./manage.py migrate data_collection [your_custom_migration] Verify all required migrations were made. As a quick check to see if you caught all schema differences in your migration, run ./ml django -mm from the. If all differences were caught, Django will return \"No changes detected\". If not, Django will make a new migration file with the updates that you need to make to your custom migration file. Save a copy of your migrated database. Run ./ml db -b to create a backup of the database in the db_backups > backups directory. The original DB, custom migration, and migrated DB should be saved together.","title":"Migrate Legacy Databases"},{"location":"developer_schema.html","text":"Mole supports the automatic generation of OpenAPI schemas. This will allow users to see what resources are avaliable via the Mole API. Generating an OpenAPI Schema \u00b6 There are two ways to generate a schema for Mole: Generate the schema using ./ml docs --generate-schema Use the docker-compose exec command. $ docker-compose exec django python manage.py generateschema --file openapi_schema.yml The generated schema will be located in /mole/openapi_schema.yml . Rendering the OpenAPI Schema to PDF \u00b6 The openapi_schema.yml file can be rendered to a human readable .pdf file using RapiPDF Clone the above repository Copy the mole/openapi_schema.yml file to the docs/specs/ directory within the RapiPDF repository Instal npm if necessary Install yarn if necessary Run yarn serve --port 8011 within the cloned RapiPDF repository Browse to http://localhost:8011 Enter ./specs/openapi_schema.yml into one of the form fields next to \"GENERATE PDF\" Click \"GENERATE PDF\"","title":"Developer schema"},{"location":"developer_schema.html#generating-an-openapi-schema","text":"There are two ways to generate a schema for Mole: Generate the schema using ./ml docs --generate-schema Use the docker-compose exec command. $ docker-compose exec django python manage.py generateschema --file openapi_schema.yml The generated schema will be located in /mole/openapi_schema.yml .","title":"Generating an OpenAPI Schema"},{"location":"developer_schema.html#rendering-the-openapi-schema-to-pdf","text":"The openapi_schema.yml file can be rendered to a human readable .pdf file using RapiPDF Clone the above repository Copy the mole/openapi_schema.yml file to the docs/specs/ directory within the RapiPDF repository Instal npm if necessary Install yarn if necessary Run yarn serve --port 8011 within the cloned RapiPDF repository Browse to http://localhost:8011 Enter ./specs/openapi_schema.yml into one of the form fields next to \"GENERATE PDF\" Click \"GENERATE PDF\"","title":"Rendering the OpenAPI Schema to PDF"},{"location":"entity_states.html","text":"Entity states add the ability to track how events affect entities, as well as modify entity appearance in the UI via point style overrides and transforms. The fields in the Entity State model describe entity point style overrides and transforms that will be serialized when an Entity is related to an Event (seen on the related_entities property on the event). EntityState \u00b6 example_entity_state = factories . EntityStateFactory ( name = \"example_state\" , point_style_icon_transform = \" {icon} _example\" , point_style_color_transform = \"#00AA00\" , point_style_use_marker_pin_override = True , point_style_marker_color_transform = \" {marker_color} 33\" , point_style_scale_factor_override = 7 , point_style_animation_transform = \"Previous animation: {animation} \" , point_style_render_as_symbol_override = True , ) Field Descriptions: name : the name of the entity state point_style_icon_transform : transforms or overrides the entity's icon, transforms must use keyword icon in curly braces. The above example would transform the icon string \"test.svg\" into \"test_example.svg\". Notice that transforms are applied before the file extension. point_style_color_transform : transforms or overrides the entity's color, transforms must use keyword color in curly braces. Colors are represented as hex strings. If you are confident that the entity type point style color property does not include an alpha value, you could use the transform to add an alpha value. Otherwise, common use for this property would be to override the color entirely. The above example would transform the color string \"#00FF00\" into \"#00AA00\". point_style_use_marker_pin_override : overrides the entity's use_marker_pin property. The above example would change the property \"False\" to \"True\". point_style_marker_color_transform : transforms or overrides the entity's marker color, transforms must use keyword marker_color in curly braces. Colors are represented as hex strings. If you are confident that the entity type point style marker color property does not include an alpha value, you could use the transform to add an alpha value. Otherwise, common use for this property would be to override the marker color entirely. The above example would transform the color string \"#00FF00\" into \"#00FF0033\". point_style_scale_factor_override : overrides the entity's scale_factor property. The above example would override the scale factor \"3\" into \"7\". point_style_animation_transform : transforms or overrides the entity's animation color, transforms must use keyword animation in curly braces. The above example would change the animation \"test\" into \"Previous animation: test\". point_style_render_as_symbol_override : overrides the entity's render_as_symbol property. The above example would change the property \"False\" to \"True\". By default, the base point style will be inherited from the point style on the entity's type. Learn more about point styles here . The fields with the _transform suffix use a Python string format to inject the previous value (from the entity's point style) into the new value using the point style property name as the format argument. The fields with the _override suffix will do a direct value override. Overrides are also possible on _transform fields by omitting the point style property reference. The state is then associated to the EntityEventRole that correlates to it. EntityEventRole \u00b6 example_role = factories . EntityEventRoleFactory ( name = \"example_role\" , metadata_key = \"example_role\" , entity_state = example_entity_state , valid_event_types = [ example_event_type ], valid_entity_types = [ example_entity_type ] valid_entity_groups = [ example_entity_group ] ) Field Descriptions: name : the name of the entity event role metadata_key : event metadata key used to relate an entity to the event entity_state : the entity state to apply to the entity related to the event valid_event_types : entities will only be related to events of an event type listed in this field, or to an event of any type if none specified valid_entity_types : entities will only be related to events if the entity's entity type is listed in this field, or will be related regardless of entity type if none specified valid_entity_groups : entities will only be related to events if the entity is in an entity group listed in this field, or will be related regardless of entity group if none specified In brief, entity event roles define the part an entity plays in an event. When an event is created, the serializer will check the event metadata for the metadata_key defined in the EntityEventRole instance and validate its value. If the value of this key matches the name of an existing entity, the serializer will attempt to relate the entity to the event and apply the state to the entity. You may optionally define entity types, event types, and entity_groups to qualify the entity for state transitions in the valid_entity_types , valid_event_types , and valid_entity_groups fields respectively. If either list is empty, a validation check will not be run on it. Validated entities will be serialized in the related_entities property on the event along with their transformed point style. The names of entities that were found in the metadata but did not pass validation will be listed in the invalid_entities property on the event. If the entity does not exist, it will be listed in the unfound_entities property on the event.","title":"Entity States"},{"location":"entity_states.html#entitystate","text":"example_entity_state = factories . EntityStateFactory ( name = \"example_state\" , point_style_icon_transform = \" {icon} _example\" , point_style_color_transform = \"#00AA00\" , point_style_use_marker_pin_override = True , point_style_marker_color_transform = \" {marker_color} 33\" , point_style_scale_factor_override = 7 , point_style_animation_transform = \"Previous animation: {animation} \" , point_style_render_as_symbol_override = True , ) Field Descriptions: name : the name of the entity state point_style_icon_transform : transforms or overrides the entity's icon, transforms must use keyword icon in curly braces. The above example would transform the icon string \"test.svg\" into \"test_example.svg\". Notice that transforms are applied before the file extension. point_style_color_transform : transforms or overrides the entity's color, transforms must use keyword color in curly braces. Colors are represented as hex strings. If you are confident that the entity type point style color property does not include an alpha value, you could use the transform to add an alpha value. Otherwise, common use for this property would be to override the color entirely. The above example would transform the color string \"#00FF00\" into \"#00AA00\". point_style_use_marker_pin_override : overrides the entity's use_marker_pin property. The above example would change the property \"False\" to \"True\". point_style_marker_color_transform : transforms or overrides the entity's marker color, transforms must use keyword marker_color in curly braces. Colors are represented as hex strings. If you are confident that the entity type point style marker color property does not include an alpha value, you could use the transform to add an alpha value. Otherwise, common use for this property would be to override the marker color entirely. The above example would transform the color string \"#00FF00\" into \"#00FF0033\". point_style_scale_factor_override : overrides the entity's scale_factor property. The above example would override the scale factor \"3\" into \"7\". point_style_animation_transform : transforms or overrides the entity's animation color, transforms must use keyword animation in curly braces. The above example would change the animation \"test\" into \"Previous animation: test\". point_style_render_as_symbol_override : overrides the entity's render_as_symbol property. The above example would change the property \"False\" to \"True\". By default, the base point style will be inherited from the point style on the entity's type. Learn more about point styles here . The fields with the _transform suffix use a Python string format to inject the previous value (from the entity's point style) into the new value using the point style property name as the format argument. The fields with the _override suffix will do a direct value override. Overrides are also possible on _transform fields by omitting the point style property reference. The state is then associated to the EntityEventRole that correlates to it.","title":"EntityState"},{"location":"entity_states.html#entityeventrole","text":"example_role = factories . EntityEventRoleFactory ( name = \"example_role\" , metadata_key = \"example_role\" , entity_state = example_entity_state , valid_event_types = [ example_event_type ], valid_entity_types = [ example_entity_type ] valid_entity_groups = [ example_entity_group ] ) Field Descriptions: name : the name of the entity event role metadata_key : event metadata key used to relate an entity to the event entity_state : the entity state to apply to the entity related to the event valid_event_types : entities will only be related to events of an event type listed in this field, or to an event of any type if none specified valid_entity_types : entities will only be related to events if the entity's entity type is listed in this field, or will be related regardless of entity type if none specified valid_entity_groups : entities will only be related to events if the entity is in an entity group listed in this field, or will be related regardless of entity group if none specified In brief, entity event roles define the part an entity plays in an event. When an event is created, the serializer will check the event metadata for the metadata_key defined in the EntityEventRole instance and validate its value. If the value of this key matches the name of an existing entity, the serializer will attempt to relate the entity to the event and apply the state to the entity. You may optionally define entity types, event types, and entity_groups to qualify the entity for state transitions in the valid_entity_types , valid_event_types , and valid_entity_groups fields respectively. If either list is empty, a validation check will not be run on it. Validated entities will be serialized in the related_entities property on the event along with their transformed point style. The names of entities that were found in the metadata but did not pass validation will be listed in the invalid_entities property on the event. If the entity does not exist, it will be listed in the unfound_entities property on the event.","title":"EntityEventRole"},{"location":"game_clock.html","text":"Game Clock \u00b6 Introduction \u00b6 Often times within an experiment, it is beneficial to have a \"game clock\" view for experiment awareness. The Game Clock is essentially a message with a clock that can count up or down from significant events or times within a scenario. The Game Clock is highly configurable, allowing the user to define clock sequences as well as responsive timers. Getting Started \u00b6 There are 3 steps to configuring the Game Clock: Define the clock phases Group phases into a clock configuration Assign clock configuration to a trial Define the Clock Phase \u00b6 The clock phase model has a number of fields that allow the game clock to be extremely configurable. Below is a table describing the options. Field Description message string : The displayed message for the clock phase message_only boolean : Set true to hide the clock string countdown boolean : true if clock counting down, false if counting up duration_seconds (optional) integer : duration of clock phase in seconds starts_with_datetime (optional) datetime : clock start time starts_with_trial_start (optional) boolean : true if clock starts with trial start_datetime starts_with_trial_end (optional) boolean : true if clock ends with trial end_datetime starts_with_event_type (optional) EventType : type of event to trigger clock phase start ends_with_datetime (optional) datetime : clock end time ends_with_trial_start (optional) boolean : true if clock ends with trial start_datetime ends_with_trial_end (optional) boolean : true if clock ends with trial end_datetime ends_with_event_type (optional) EventType : type of event to trigger clock phase end Currently clock phases can be pre-configured in the Mole configuration script, or they can be posted to the API. Group Phases in Clock Configuration \u00b6 The clock config model simply defines a timezone and a list of phases. The order of the phases in the list does not matter, Mole will infer the most appropriate phase given the state of the experiment and defined clock phases. Assign Clock Configuration to Trial \u00b6 The trial model has a clock config foreign relation, simply set this field with a reference to the clock config. Example Game Clock Configuration \u00b6 # define the clock phases trial_phase_1 = factories . ClockPhaseFactory ( message = \"Standing By\" , message_only = True , ends_with_trial_start = True ) trial_phase_2 = factories . ClockPhaseFactory ( message = \"Time Until Setup\" , countdown = True , ends_with_trial_start = True , duration_seconds = 900 ) trial_phase_3 = factories . ClockPhaseFactory ( message = \"Pre-Run Checkout\" , countdown = True , starts_with_trial_start = True , duration_seconds = 900 ) trial_phase_4 = factories . ClockPhaseFactory ( message = \"Team Setup\" , message_only = True , starts_with_trial_end = True ) trial_phase_5 = factories . ClockPhaseFactory ( message = \"5 minute countdown from maintenance stop event:\" , countdown = True , duration_seconds = 300 , starts_with_event_type = maintenance_stop_event_type ) # define the clock configuration trial_clock = factories . ClockConfigFactory ( name = \"Trial Clock\" , timezone = \"America/Los_Angeles\" ) # add phases to clock configuration trial_clock . phases . add ( trial_phase_1 , trial_phase_2 , trial_phase_3 , trial_phase_4 , trial_phase_5 ) Multiple Clock Instances \u00b6 The Game Clock allows multiple clock instances to run at the same time. By default, the primary clock will reference the clock configuration assigned to the current trial. There are a few different clocks that can run parallel to the current trial clock: Reported Clock \u00b6 The Reported Clock is the clock that always tracks the first \"Reported\" trial that has the same major and minor IDs as the current trial. Minor Clock \u00b6 The Minor Clock is the clock assigned to the \"Minor Trial\" that is related to the current trial. The Minor Trial has the same major and minor IDs as the current trial, but a zero for the micro ID. Example: if the current trial's ID is 2.2.1, the Minor Trial's ID is 2.2.0. Major Clock \u00b6 The Major Clock is the clock assigned to the \"Major Trial\" that is related to the current trial. The Major Trial has the same major ID as the current trial, but zeros for the minor and micro IDs. Example: if the current trial's ID is 2.2.1, the Minor Trial's ID is 2.0.0. Angular Distinctions \u00b6 In Angular, these different clocks are distinguished by the mole-timer-card input: <!-- Current trial clock --> < mole-timer-card ></ mole-timer-card > <!-- Reported trial clock --> < mole-timer-card [ reported ]=\" true \" ></ mole-timer-card > <!-- Minor trial clock --> < mole-timer-card [ minor ]=\" true \" ></ mole-timer-card > <!-- Major trial clock --> < mole-timer-card [ major ]=\" true \" ></ mole-timer-card >","title":"Game Clock"},{"location":"game_clock.html#game-clock","text":"","title":"Game Clock"},{"location":"game_clock.html#introduction","text":"Often times within an experiment, it is beneficial to have a \"game clock\" view for experiment awareness. The Game Clock is essentially a message with a clock that can count up or down from significant events or times within a scenario. The Game Clock is highly configurable, allowing the user to define clock sequences as well as responsive timers.","title":"Introduction"},{"location":"game_clock.html#getting-started","text":"There are 3 steps to configuring the Game Clock: Define the clock phases Group phases into a clock configuration Assign clock configuration to a trial","title":"Getting Started"},{"location":"game_clock.html#define-the-clock-phase","text":"The clock phase model has a number of fields that allow the game clock to be extremely configurable. Below is a table describing the options. Field Description message string : The displayed message for the clock phase message_only boolean : Set true to hide the clock string countdown boolean : true if clock counting down, false if counting up duration_seconds (optional) integer : duration of clock phase in seconds starts_with_datetime (optional) datetime : clock start time starts_with_trial_start (optional) boolean : true if clock starts with trial start_datetime starts_with_trial_end (optional) boolean : true if clock ends with trial end_datetime starts_with_event_type (optional) EventType : type of event to trigger clock phase start ends_with_datetime (optional) datetime : clock end time ends_with_trial_start (optional) boolean : true if clock ends with trial start_datetime ends_with_trial_end (optional) boolean : true if clock ends with trial end_datetime ends_with_event_type (optional) EventType : type of event to trigger clock phase end Currently clock phases can be pre-configured in the Mole configuration script, or they can be posted to the API.","title":"Define the Clock Phase"},{"location":"game_clock.html#group-phases-in-clock-configuration","text":"The clock config model simply defines a timezone and a list of phases. The order of the phases in the list does not matter, Mole will infer the most appropriate phase given the state of the experiment and defined clock phases.","title":"Group Phases in Clock Configuration"},{"location":"game_clock.html#assign-clock-configuration-to-trial","text":"The trial model has a clock config foreign relation, simply set this field with a reference to the clock config.","title":"Assign Clock Configuration to Trial"},{"location":"game_clock.html#example-game-clock-configuration","text":"# define the clock phases trial_phase_1 = factories . ClockPhaseFactory ( message = \"Standing By\" , message_only = True , ends_with_trial_start = True ) trial_phase_2 = factories . ClockPhaseFactory ( message = \"Time Until Setup\" , countdown = True , ends_with_trial_start = True , duration_seconds = 900 ) trial_phase_3 = factories . ClockPhaseFactory ( message = \"Pre-Run Checkout\" , countdown = True , starts_with_trial_start = True , duration_seconds = 900 ) trial_phase_4 = factories . ClockPhaseFactory ( message = \"Team Setup\" , message_only = True , starts_with_trial_end = True ) trial_phase_5 = factories . ClockPhaseFactory ( message = \"5 minute countdown from maintenance stop event:\" , countdown = True , duration_seconds = 300 , starts_with_event_type = maintenance_stop_event_type ) # define the clock configuration trial_clock = factories . ClockConfigFactory ( name = \"Trial Clock\" , timezone = \"America/Los_Angeles\" ) # add phases to clock configuration trial_clock . phases . add ( trial_phase_1 , trial_phase_2 , trial_phase_3 , trial_phase_4 , trial_phase_5 )","title":"Example Game Clock Configuration"},{"location":"game_clock.html#multiple-clock-instances","text":"The Game Clock allows multiple clock instances to run at the same time. By default, the primary clock will reference the clock configuration assigned to the current trial. There are a few different clocks that can run parallel to the current trial clock:","title":"Multiple Clock Instances"},{"location":"game_clock.html#reported-clock","text":"The Reported Clock is the clock that always tracks the first \"Reported\" trial that has the same major and minor IDs as the current trial.","title":"Reported Clock"},{"location":"game_clock.html#minor-clock","text":"The Minor Clock is the clock assigned to the \"Minor Trial\" that is related to the current trial. The Minor Trial has the same major and minor IDs as the current trial, but a zero for the micro ID. Example: if the current trial's ID is 2.2.1, the Minor Trial's ID is 2.2.0.","title":"Minor Clock"},{"location":"game_clock.html#major-clock","text":"The Major Clock is the clock assigned to the \"Major Trial\" that is related to the current trial. The Major Trial has the same major ID as the current trial, but zeros for the minor and micro IDs. Example: if the current trial's ID is 2.2.1, the Minor Trial's ID is 2.0.0.","title":"Major Clock"},{"location":"game_clock.html#angular-distinctions","text":"In Angular, these different clocks are distinguished by the mole-timer-card input: <!-- Current trial clock --> < mole-timer-card ></ mole-timer-card > <!-- Reported trial clock --> < mole-timer-card [ reported ]=\" true \" ></ mole-timer-card > <!-- Minor trial clock --> < mole-timer-card [ minor ]=\" true \" ></ mole-timer-card > <!-- Major trial clock --> < mole-timer-card [ major ]=\" true \" ></ mole-timer-card >","title":"Angular Distinctions"},{"location":"getting_started.html","text":"Getting Started \u00b6 Requirements \u00b6 1) Docker Add docker apt repository, install Docker, create docker group and add the user to it, then start the service. See https://docs.docker.com/engine/install/ubuntu/ $ sudo apt-get update $ sudo sudo apt-get install apt-transport-https ca-certificates curl gnupg lsb-release $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg $ echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null $ sudo apt-get update $ sudo apt-get install docker-ce docker-ce-cli containerd.io $ sudo groupadd docker $ sudo usermod -aG docker $USER $ sudo service docker start Reboot, then confirm docker is running. $ docker run hello-world Note If building the Docker containers hangs in the Mole initialization step below, see Configuring Docker to Use a Different DNS Server . This may be due to Docker's default DNS server (Google's 8.8.8.8) being blocked on your network. 2) docker-compose Install Docker-Compose locally, set permissions, and modify the path. See https://docs.docker.com/compose/install/ $ sudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose $ sudo chmod +x /usr/local/bin/docker-compose $ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose Confirm that docker-compose is installed $ docker-compose --version Note In order for Mole to automatically generate keys/certificates for https, the openssl command must be installed on the host. This is generally the case by default. Services \u00b6 Mole is composed of a number of different services including the following: proxy : A Traefik-based reverse proxy to expose services at paths and via https. Accessible on host port 8080 ( http://localhost:8080 ) postgres : A PostgreSQL database server. Accessible on host port 5432. pulsar : An Apache Pulsar server. Pulsar is the native messaging platform for Mole. Accessible on host ports 6650. The Admin API is available at http://localhost:8090 django : A Django-based REST API server. It also serves static frontend content. Accessible on host port 8000 (e.g., http://localhost:8000 ) or domain root (e.g., http://localhost ) - Note: A browseable API is also available at /api/. (e.g., http://localhost/api/ ) angular : An Angular development server. As changes are made in the code, the server live-updates the UI. Accessible on host port 4200 (e.g., http://localhost:4200 ) or at the path http://localhost/angular/ - Note: Only accessible after running ./ml ang or -a flag on run script redis : A Redis server. - Note: By default the redis service is not exposed outside of the Docker network. To expose Redis externally, use the --unlock-redis command line option to ./ml init or ./ml run or set the UNLOCK_REDIS environment variable on the host machine. When exposed, Redis will be accessible on host port 6379. docs : A Mkdocs-based documentation server. Accessible on host port 8001 (e.g., http://localhost:8001 ) or at the path http://localhost/docs/ maptiles : A TileServerGL-based map tile server. Accessible on host port 8081 (e.g., http://localhost:8081 ) or at the path http://localhost/maps/ report : A Plotly Dash-based data visualization and report server. Accessible on host port 8400 (e.g., http://localhost:8400 ) or at the path http://localhost/report/ portainer : A Portainer Docker management and monitoring server. Accessible on host port 9000 (e.g., http://localhost:9000 ) or at the path http://localhost/portainer/ with: - user: admin - password: password Https (optional) \u00b6 All http endpoints can be made available via https. Certificates/keys are automatically generated the first time ./ml init is run. See below to generate new keys. Adding the Mole Certificate Authority certificate to Chrome \u00b6 Warning Please ensure you understand the security implications before importing the Mole Certificate Authority into your browser. In order to \"trust\" the Mole certificate, perform the following: Open Chrome Settings Select Security Select Manage Certificates Select Authorities Click Import Browse to the location of the Mole repository under mole/traefik/certificates Select moleCA.pem Click Trust this certificate for identifying websites If you are trying to \"trust\" the Mole certificate on a machine other than the Mole server, first copy the mole/traefik/certificates/moleCA.pem file from the Mole server to an accssible location, then browse to this location when selecting the certificate to import. Note Once the Mole Certificate Authority certificate has been imported it should be at the top of the list as org-_Mole . If the CA certificate is updated in the server, it must be replaced in the browser as well. Generating keys \u00b6 Although ./ml init automatically generates keys/certificates the first time it is run, it may be useful to replace or update them for various reasons. The ./ml script includes a helper to generate new keys: ./ml keys . This helper has options for selectively generating only the certificate authority ( ./ml keys --ca ) or the server ( ./ml keys --server ) keys. This is useful, for example, to re-generate the server keys while keeping the same certificate authority. In order for the certificate to be valid for hosts or IPs other than localhost or 127.0.0.1 , the desired hosts/IPs must be added to traefik/configuration/cert.ext under the alt_names section. Once the cert.ext file has been updated with new hosts/IPs, a new server certificate must be generated using ./ml key --server . This will generate a new \"server\" certificate using the existing Certificate Authority. I.e., no new Certificate Authority key/cert is generated when the --server flag is used. Note Once the server certificate has been updated with new hosts/IPs, Chrome should automatically trust the new one for those IPs. I.e., there is no need to re-import the Mole Certificate Authority in Chrome unless it was re-generated as well. Running \u00b6 The ml script at the root of the Mole repo can be used to build containers, start services, populate dbs, stop services, or build and serve documentation. It is structured with a number of subcommands. Additional help on each command can be found by passing the -h flag. E.g., ./ml run -h . If this is the first time you are running Mole, use the following command to configure Mole and initialize the database with defaults: $./ml init Warning Mole is currently intended to be run within a trusted network. It has not yet been vetted for open-internet deployment. Subsequent runs can use: $./ml run See below for additional information on the ./ml command and examples of its use. Note If accessing the Mole dashboard produces an HTTP 500 error, the front end (Angular) files may not have been built. These can be built with the following command: $ ./ml ang -b Commands \u00b6 init : Create the Docker containers and initialize a database based on the available configuration scripts. If no CONFIGURE_SCRIPT is specified, \"configure_mole\" is used. - use -a flag to spin up angular development server - use -s to skip the static front-end build process - use --deep-clean to clear containers and volumes before init process keys : Generate CA or server keys/certificates for https. run : Runs a pre-configured container. - use -q flag to run as a daemon. - use --lite flag to omit running superflous containters (e.g., portainer, docs, etc.) - use -a flag to spin up angular development server stop : Stops all containers. test : Runs the Django unit tests. Note: Mole does not run with this command. shell : Brings up all containers and starts interactive shell. Note: Mole does not run in this mode. docs : Build documentation (including OpenAPI schema and graphing database models). Documentation is served at http://localhost:8001 or http://docs.localhost . maps : Serve map tiles at http://localhost:8081 or http://maps.localhost . Note: Mole does not run with this command. See Docs for more information. db : Saves and loads database + media backup archives. - use -b flag to create a backup archive - use -l flag to load a backup archive management : Load/Save docker images to or from an archive file. This archive file can be used to export docker images for use on external machines. Also builds containers from images in the local repo. django : Provides set of tools to interact with Django container. Currently supports the makemigrations command with -mm flag. ang : Spins up Angular development server. - use -b flag to build the angular files to be served statically from Django Examples \u00b6 The following command will build containers and populate the database with initial data using the default \"configure_mole\" script: $./ml init Note See below if building the containers hangs. This may be due to Docker's default DNS server (Google's 8.8.8.8) being blocked on your network. The following command will build containers and populate the database with initial data using a custom \"configure_mole\" script (configure_mole_test): $./ml init configure_mole_test The following command will run previously built containers as a daemon: $./ml run -q Mole is available at http://localhost . The browseable API is available at http://localhost/api Tip Documentation is also accessible at http://localhost/docs when Mole is running. The following command will run previously built containers, but not run the map tile server: $./ml run --nomaps The following command will stop running containers: $./ml stop The following command will spin up the Angular development server: $./ml ang The following command will build the Angular files to be served statically from Django: $./ml ang -b The following command will build and serve the Mole documentation: $./ml docs The following command will run the Mole map tile server alone: $./ml maps Note See maps section of documentation for information on creating tiles and configuring the map server. The following command will run the automated tests: $./ml test The following command will build Django migration files if schema changes have been made: $./ml django -mm Database Backups \u00b6 By default, Mole produces database backups before an init and before a database is loaded, however it has the capability to perform the automatic backups listed below if the -db flag is used on the init or run commands. On startup ( ./ml run -db ) On shutdown ( ^c or ./ml stop ) Periodically (at the top of every hour) Regardless of the -db flag being used, backups can be created at anytime using the command below. On demand ( ./ml db -b ) Tip If you would like to not perform a database backup prior to an init, you can use the -nb flag on the init command: ./ml init -nb Tip On-demand backups ( ./ml db -b ) can be created with Mole running or stopped; services will be returned to their pre-backup state. In all cases, backups are ultimately launched via a web service. A GET request to http://localhost:8003/backup_db/ or http://backup.localhost/backup_db/ will launch a database backup job. This service is throttled to approximately one request per minute. Tip The database backup web service includes an interactive API documentation served at http://localhost:8003/docs/ or http://localhost/db_backup/docs/ There are two optional querystring parameters for the backup service: context represents a string that will be appended to the end of the backup filename to indicate the context for the creation of the backup. (e.g., \"startup\", \"shutdown\", \"on-demand\", etc.). For example a GET request to http://localhost/db_backup/backup_db/?context=docs (via browser, curl, or otherwise) would initiate a database backup with _docs appended to the end of the filename. sync indicates that the request shouldn't return a response until the backup has been completed. This is useful if a subsequent action (e.g., clearing the database) needs to ensure the backup has been completed prior to preceeding. Normally the backup call returns immediately and launches the database backup job in the background. Example: http://localhost/db_backup/backup_db/?sync=true . Note To include both context and sync querystrings, separate them with an ampersand ( & ) symbol, e.g., http://localhost/db_backup/backup_db/?context=docs&sync=true . If this is done from a terminal curl command, ensure to wrap the url in quotation marks so the & isn't interpreted as a request to background the job. Example: curl \"http://localhost/db_backup/backup_db/?context=docs&sync=true\" Database Backup Location \u00b6 Saved database archives are stored in the db_backup/backups/ directory. The naming convention is mole_backup_<year>-<month>-<day>_<hour>_<minute>-<second>_<stage>.sql where <stage> is a string that indicates the reason the database file was created (e.g., \"start_up\", \"shutdown\", \"pre-init\", \"pre-load\", \"on_demand\", \"periodic\") Restoring from a Database Backup Archive \u00b6 Database files can be restored using the following command: ./ml db -l <archive-name> where the location of <archive-name> is assumed to be in db_backup/backups/ . Mole supports loading standalone sql backups, as well as the default archive format. Mole is also able to import backups from outside of the db_backups/backups directory if the absolute filepath is provided. Exporting Docker Images to an External Host \u00b6 To run the Docker containers you must have Docker images in a local repo. Normally, these containers are built and configured when we execute the init command. However, when a host is unable to access outside networks, it will be unable to build images on initialization. You can load archived images with the ml manage command, and run preconfigured images. Note This feature is currently unable to export on machines with differing architectures. Process: 1) Save project images that have already been populated and constructed. To save all project Docker images into a single archive use the following command ./ml manage -s this will save the project Docker images to archive mole_project.tar.gz 2) You may now export the archive to another host. Simply save to external media or transfer if you have that option. 3) Load the images on the new machine: ./ml manage -l mole_project.tar.gz 4) Build the containers: ./ml manage -b or ./ml run Configuring Docker to Use a Different DNS Server \u00b6 To configure Docker on Ubuntu to use the DNS server listed, edit /lib/systemd/system/docker.service and replace the line ExecStart=/usr/bin/dockerd -H fd:// with ExecStart=/usr/bin/dockerd --dns <desired_DNS> -H fd:// You can find out what your current DNS server is with the following command: nmcli dev show | grep 'IP4.DNS' Note This is only necessary if building the container images hangs due to the default DNS server being blocked on your network.","title":"Getting Started"},{"location":"getting_started.html#getting-started","text":"","title":"Getting Started"},{"location":"getting_started.html#requirements","text":"1) Docker Add docker apt repository, install Docker, create docker group and add the user to it, then start the service. See https://docs.docker.com/engine/install/ubuntu/ $ sudo apt-get update $ sudo sudo apt-get install apt-transport-https ca-certificates curl gnupg lsb-release $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg $ echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null $ sudo apt-get update $ sudo apt-get install docker-ce docker-ce-cli containerd.io $ sudo groupadd docker $ sudo usermod -aG docker $USER $ sudo service docker start Reboot, then confirm docker is running. $ docker run hello-world Note If building the Docker containers hangs in the Mole initialization step below, see Configuring Docker to Use a Different DNS Server . This may be due to Docker's default DNS server (Google's 8.8.8.8) being blocked on your network. 2) docker-compose Install Docker-Compose locally, set permissions, and modify the path. See https://docs.docker.com/compose/install/ $ sudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose $ sudo chmod +x /usr/local/bin/docker-compose $ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose Confirm that docker-compose is installed $ docker-compose --version Note In order for Mole to automatically generate keys/certificates for https, the openssl command must be installed on the host. This is generally the case by default.","title":"Requirements"},{"location":"getting_started.html#services","text":"Mole is composed of a number of different services including the following: proxy : A Traefik-based reverse proxy to expose services at paths and via https. Accessible on host port 8080 ( http://localhost:8080 ) postgres : A PostgreSQL database server. Accessible on host port 5432. pulsar : An Apache Pulsar server. Pulsar is the native messaging platform for Mole. Accessible on host ports 6650. The Admin API is available at http://localhost:8090 django : A Django-based REST API server. It also serves static frontend content. Accessible on host port 8000 (e.g., http://localhost:8000 ) or domain root (e.g., http://localhost ) - Note: A browseable API is also available at /api/. (e.g., http://localhost/api/ ) angular : An Angular development server. As changes are made in the code, the server live-updates the UI. Accessible on host port 4200 (e.g., http://localhost:4200 ) or at the path http://localhost/angular/ - Note: Only accessible after running ./ml ang or -a flag on run script redis : A Redis server. - Note: By default the redis service is not exposed outside of the Docker network. To expose Redis externally, use the --unlock-redis command line option to ./ml init or ./ml run or set the UNLOCK_REDIS environment variable on the host machine. When exposed, Redis will be accessible on host port 6379. docs : A Mkdocs-based documentation server. Accessible on host port 8001 (e.g., http://localhost:8001 ) or at the path http://localhost/docs/ maptiles : A TileServerGL-based map tile server. Accessible on host port 8081 (e.g., http://localhost:8081 ) or at the path http://localhost/maps/ report : A Plotly Dash-based data visualization and report server. Accessible on host port 8400 (e.g., http://localhost:8400 ) or at the path http://localhost/report/ portainer : A Portainer Docker management and monitoring server. Accessible on host port 9000 (e.g., http://localhost:9000 ) or at the path http://localhost/portainer/ with: - user: admin - password: password","title":"Services"},{"location":"getting_started.html#https-optional","text":"All http endpoints can be made available via https. Certificates/keys are automatically generated the first time ./ml init is run. See below to generate new keys.","title":"Https (optional)"},{"location":"getting_started.html#adding-the-mole-certificate-authority-certificate-to-chrome","text":"Warning Please ensure you understand the security implications before importing the Mole Certificate Authority into your browser. In order to \"trust\" the Mole certificate, perform the following: Open Chrome Settings Select Security Select Manage Certificates Select Authorities Click Import Browse to the location of the Mole repository under mole/traefik/certificates Select moleCA.pem Click Trust this certificate for identifying websites If you are trying to \"trust\" the Mole certificate on a machine other than the Mole server, first copy the mole/traefik/certificates/moleCA.pem file from the Mole server to an accssible location, then browse to this location when selecting the certificate to import. Note Once the Mole Certificate Authority certificate has been imported it should be at the top of the list as org-_Mole . If the CA certificate is updated in the server, it must be replaced in the browser as well.","title":"Adding the Mole Certificate Authority certificate to Chrome"},{"location":"getting_started.html#generating-keys","text":"Although ./ml init automatically generates keys/certificates the first time it is run, it may be useful to replace or update them for various reasons. The ./ml script includes a helper to generate new keys: ./ml keys . This helper has options for selectively generating only the certificate authority ( ./ml keys --ca ) or the server ( ./ml keys --server ) keys. This is useful, for example, to re-generate the server keys while keeping the same certificate authority. In order for the certificate to be valid for hosts or IPs other than localhost or 127.0.0.1 , the desired hosts/IPs must be added to traefik/configuration/cert.ext under the alt_names section. Once the cert.ext file has been updated with new hosts/IPs, a new server certificate must be generated using ./ml key --server . This will generate a new \"server\" certificate using the existing Certificate Authority. I.e., no new Certificate Authority key/cert is generated when the --server flag is used. Note Once the server certificate has been updated with new hosts/IPs, Chrome should automatically trust the new one for those IPs. I.e., there is no need to re-import the Mole Certificate Authority in Chrome unless it was re-generated as well.","title":"Generating keys"},{"location":"getting_started.html#running","text":"The ml script at the root of the Mole repo can be used to build containers, start services, populate dbs, stop services, or build and serve documentation. It is structured with a number of subcommands. Additional help on each command can be found by passing the -h flag. E.g., ./ml run -h . If this is the first time you are running Mole, use the following command to configure Mole and initialize the database with defaults: $./ml init Warning Mole is currently intended to be run within a trusted network. It has not yet been vetted for open-internet deployment. Subsequent runs can use: $./ml run See below for additional information on the ./ml command and examples of its use. Note If accessing the Mole dashboard produces an HTTP 500 error, the front end (Angular) files may not have been built. These can be built with the following command: $ ./ml ang -b","title":"Running"},{"location":"getting_started.html#commands","text":"init : Create the Docker containers and initialize a database based on the available configuration scripts. If no CONFIGURE_SCRIPT is specified, \"configure_mole\" is used. - use -a flag to spin up angular development server - use -s to skip the static front-end build process - use --deep-clean to clear containers and volumes before init process keys : Generate CA or server keys/certificates for https. run : Runs a pre-configured container. - use -q flag to run as a daemon. - use --lite flag to omit running superflous containters (e.g., portainer, docs, etc.) - use -a flag to spin up angular development server stop : Stops all containers. test : Runs the Django unit tests. Note: Mole does not run with this command. shell : Brings up all containers and starts interactive shell. Note: Mole does not run in this mode. docs : Build documentation (including OpenAPI schema and graphing database models). Documentation is served at http://localhost:8001 or http://docs.localhost . maps : Serve map tiles at http://localhost:8081 or http://maps.localhost . Note: Mole does not run with this command. See Docs for more information. db : Saves and loads database + media backup archives. - use -b flag to create a backup archive - use -l flag to load a backup archive management : Load/Save docker images to or from an archive file. This archive file can be used to export docker images for use on external machines. Also builds containers from images in the local repo. django : Provides set of tools to interact with Django container. Currently supports the makemigrations command with -mm flag. ang : Spins up Angular development server. - use -b flag to build the angular files to be served statically from Django","title":"Commands"},{"location":"getting_started.html#examples","text":"The following command will build containers and populate the database with initial data using the default \"configure_mole\" script: $./ml init Note See below if building the containers hangs. This may be due to Docker's default DNS server (Google's 8.8.8.8) being blocked on your network. The following command will build containers and populate the database with initial data using a custom \"configure_mole\" script (configure_mole_test): $./ml init configure_mole_test The following command will run previously built containers as a daemon: $./ml run -q Mole is available at http://localhost . The browseable API is available at http://localhost/api Tip Documentation is also accessible at http://localhost/docs when Mole is running. The following command will run previously built containers, but not run the map tile server: $./ml run --nomaps The following command will stop running containers: $./ml stop The following command will spin up the Angular development server: $./ml ang The following command will build the Angular files to be served statically from Django: $./ml ang -b The following command will build and serve the Mole documentation: $./ml docs The following command will run the Mole map tile server alone: $./ml maps Note See maps section of documentation for information on creating tiles and configuring the map server. The following command will run the automated tests: $./ml test The following command will build Django migration files if schema changes have been made: $./ml django -mm","title":"Examples"},{"location":"getting_started.html#database-backups","text":"By default, Mole produces database backups before an init and before a database is loaded, however it has the capability to perform the automatic backups listed below if the -db flag is used on the init or run commands. On startup ( ./ml run -db ) On shutdown ( ^c or ./ml stop ) Periodically (at the top of every hour) Regardless of the -db flag being used, backups can be created at anytime using the command below. On demand ( ./ml db -b ) Tip If you would like to not perform a database backup prior to an init, you can use the -nb flag on the init command: ./ml init -nb Tip On-demand backups ( ./ml db -b ) can be created with Mole running or stopped; services will be returned to their pre-backup state. In all cases, backups are ultimately launched via a web service. A GET request to http://localhost:8003/backup_db/ or http://backup.localhost/backup_db/ will launch a database backup job. This service is throttled to approximately one request per minute. Tip The database backup web service includes an interactive API documentation served at http://localhost:8003/docs/ or http://localhost/db_backup/docs/ There are two optional querystring parameters for the backup service: context represents a string that will be appended to the end of the backup filename to indicate the context for the creation of the backup. (e.g., \"startup\", \"shutdown\", \"on-demand\", etc.). For example a GET request to http://localhost/db_backup/backup_db/?context=docs (via browser, curl, or otherwise) would initiate a database backup with _docs appended to the end of the filename. sync indicates that the request shouldn't return a response until the backup has been completed. This is useful if a subsequent action (e.g., clearing the database) needs to ensure the backup has been completed prior to preceeding. Normally the backup call returns immediately and launches the database backup job in the background. Example: http://localhost/db_backup/backup_db/?sync=true . Note To include both context and sync querystrings, separate them with an ampersand ( & ) symbol, e.g., http://localhost/db_backup/backup_db/?context=docs&sync=true . If this is done from a terminal curl command, ensure to wrap the url in quotation marks so the & isn't interpreted as a request to background the job. Example: curl \"http://localhost/db_backup/backup_db/?context=docs&sync=true\"","title":"Database Backups"},{"location":"getting_started.html#database-backup-location","text":"Saved database archives are stored in the db_backup/backups/ directory. The naming convention is mole_backup_<year>-<month>-<day>_<hour>_<minute>-<second>_<stage>.sql where <stage> is a string that indicates the reason the database file was created (e.g., \"start_up\", \"shutdown\", \"pre-init\", \"pre-load\", \"on_demand\", \"periodic\")","title":"Database Backup Location"},{"location":"getting_started.html#restoring-from-a-database-backup-archive","text":"Database files can be restored using the following command: ./ml db -l <archive-name> where the location of <archive-name> is assumed to be in db_backup/backups/ . Mole supports loading standalone sql backups, as well as the default archive format. Mole is also able to import backups from outside of the db_backups/backups directory if the absolute filepath is provided.","title":"Restoring from a Database Backup Archive"},{"location":"getting_started.html#exporting-docker-images-to-an-external-host","text":"To run the Docker containers you must have Docker images in a local repo. Normally, these containers are built and configured when we execute the init command. However, when a host is unable to access outside networks, it will be unable to build images on initialization. You can load archived images with the ml manage command, and run preconfigured images. Note This feature is currently unable to export on machines with differing architectures. Process: 1) Save project images that have already been populated and constructed. To save all project Docker images into a single archive use the following command ./ml manage -s this will save the project Docker images to archive mole_project.tar.gz 2) You may now export the archive to another host. Simply save to external media or transfer if you have that option. 3) Load the images on the new machine: ./ml manage -l mole_project.tar.gz 4) Build the containers: ./ml manage -b or ./ml run","title":"Exporting Docker Images to an External Host"},{"location":"getting_started.html#configuring-docker-to-use-a-different-dns-server","text":"To configure Docker on Ubuntu to use the DNS server listed, edit /lib/systemd/system/docker.service and replace the line ExecStart=/usr/bin/dockerd -H fd:// with ExecStart=/usr/bin/dockerd --dns <desired_DNS> -H fd:// You can find out what your current DNS server is with the following command: nmcli dev show | grep 'IP4.DNS' Note This is only necessary if building the container images hangs due to the default DNS server being blocked on your network.","title":"Configuring Docker to Use a Different DNS Server"},{"location":"maps.html","text":"Maps \u00b6 TODO Add section on configuring tileserver-gl with json file to serve multiple tilesets and to serve vector tiles. Mole includes a service that serves map tiles using tileserver-gl . To serve an set of map tiles in the .mbtiles format, place the file in the maps/maptiles folder. Note The .mbtiles file must have the name and format metadata tables set. See the .mbtiles spec for more information. The instructions below describe adding the appropriate metadata. The tiles will be served at http://localhost/maps/data/<filename_without_extension>/<z>/<y>/<x>.jpg A map preview is served at http://localhost/maps/data/<filename_without_extension> The maps service runs by default. To disable it, use the --nomaps flag to ./ml run : ./ml run --nomaps Generating Aerial Imagery Map Tiles \u00b6 This section describes methods for obtaining aerial imagery and processing it into an .mbtiles file to be served by the Mole map tile server. Acquiring Aerial Imagery \u00b6 Digital Globe Imagery (preferred method) \u00b6 Government employees can get access to Digital Globe imagery. This generally provides more recent data than that available via the NGA national map site. If you have access to Digital Globe, log in at https://evwhs.digitalglobe.com . Here you can select a region of interest to download. Define an area of interest with the drawing tool. Select Generate a tileset Enter Name Set \"End Zoom Level\" to 18 Click <Save> Files will be available under My Imagery --> Library once they have been prepared (takes a while) Uncompress the archive National Map Imagery (method 2) \u00b6 Go to http://viewer.nationalmap.gov/basic/ to download imagery for region of interest. Select \"Imagery - 1 foot (HRO)\" at the left Zoom to desired bounding box Click the black square icon Draw desired bounding box. Click the \"Find Products\" button at the top of the left pane. You can add additional search options (e.g. \"CL\" for only 3 band color imagery or \"4B\" for 4 band color imagery) Add all items to the card by pressing the \"+All\" icon for each page in the search returns This is tedious. Please update this page if you find a better way. Click \"View Cart\" Click \"List and Export Cart Items\" This will spawn a file download that corresponds to a .csv file with download links and information about each of the requested images. Download the files in the .csv file by either using the download manager (java program) Unzip all downloaded files. This should result in a set of .jp2 files. Processing Imagery into .mbtiles \u00b6 Digital Globe Imagery \u00b6 The archive downloaded above already has a tiled structure. We just need to create an mbtiles file. The mbutil utility can do this. Install mbutil git clone http://github.com/mapbox/mbutil.git Change directory to the base of the Digital Globe tileset Sub-directories at this level should be numbers (e.g., /12/) Create metadata.json file: echo '{\"name\": \"<desired_tileset_name>\", \"format\": \"jpg\"}' > metadata.json Create the .mbtiles file: mbutil/mb-util --image_format=jpg ./ <filename>.mbtiles National Map Imagery (method 2) \u00b6 In the folder containing the downloaded .jp2 files, run the following commands: gdalbuildvrt index.vrt *.jp2 gdalwarp -r near --config GDAL_CACHEMAX 3000 -wm 3000 -co compress=jpeg -co photometric=ycbcr -co tiled=yes -t_srs EPSG:3857 index.vrt <filename>.tif see http://www.gdal.org/gdalwarp.html for more options, particularly for the resampling method (-r flag) gdaladdo -r nearest --config COMPRESS_OVERVIEW JPEG --config PHOTOMETRIC_OVERVIEW YCBCR <filename>.tif 2 4 8 16 32 64 128 see http://www.gdal.org/gdaladdo.html for more options, particularly for the resampling method (-r flag) Use gdal2tiles.py to cut the tiles. Install gdal2tiles.py sudo apt-get install python-gdal gdal2tiles.py <filename>.tif This will create a folder called with appropriate tiles in it Use mbutil to create the mbtiles file Install mbutil git clone http://github.com/mapbox/mbutil.git ./mbutil/mb-util --scheme=tms <filename>/ <filename>.mbtiles","title":"Maps"},{"location":"maps.html#maps","text":"TODO Add section on configuring tileserver-gl with json file to serve multiple tilesets and to serve vector tiles. Mole includes a service that serves map tiles using tileserver-gl . To serve an set of map tiles in the .mbtiles format, place the file in the maps/maptiles folder. Note The .mbtiles file must have the name and format metadata tables set. See the .mbtiles spec for more information. The instructions below describe adding the appropriate metadata. The tiles will be served at http://localhost/maps/data/<filename_without_extension>/<z>/<y>/<x>.jpg A map preview is served at http://localhost/maps/data/<filename_without_extension> The maps service runs by default. To disable it, use the --nomaps flag to ./ml run : ./ml run --nomaps","title":"Maps"},{"location":"maps.html#generating-aerial-imagery-map-tiles","text":"This section describes methods for obtaining aerial imagery and processing it into an .mbtiles file to be served by the Mole map tile server.","title":"Generating Aerial Imagery Map Tiles"},{"location":"maps.html#acquiring-aerial-imagery","text":"","title":"Acquiring Aerial Imagery"},{"location":"maps.html#digital-globe-imagery-preferred-method","text":"Government employees can get access to Digital Globe imagery. This generally provides more recent data than that available via the NGA national map site. If you have access to Digital Globe, log in at https://evwhs.digitalglobe.com . Here you can select a region of interest to download. Define an area of interest with the drawing tool. Select Generate a tileset Enter Name Set \"End Zoom Level\" to 18 Click <Save> Files will be available under My Imagery --> Library once they have been prepared (takes a while) Uncompress the archive","title":"Digital Globe Imagery (preferred method)"},{"location":"maps.html#national-map-imagery-method-2","text":"Go to http://viewer.nationalmap.gov/basic/ to download imagery for region of interest. Select \"Imagery - 1 foot (HRO)\" at the left Zoom to desired bounding box Click the black square icon Draw desired bounding box. Click the \"Find Products\" button at the top of the left pane. You can add additional search options (e.g. \"CL\" for only 3 band color imagery or \"4B\" for 4 band color imagery) Add all items to the card by pressing the \"+All\" icon for each page in the search returns This is tedious. Please update this page if you find a better way. Click \"View Cart\" Click \"List and Export Cart Items\" This will spawn a file download that corresponds to a .csv file with download links and information about each of the requested images. Download the files in the .csv file by either using the download manager (java program) Unzip all downloaded files. This should result in a set of .jp2 files.","title":"National Map Imagery (method 2)"},{"location":"maps.html#processing-imagery-into-mbtiles","text":"","title":"Processing Imagery into .mbtiles"},{"location":"maps.html#digital-globe-imagery","text":"The archive downloaded above already has a tiled structure. We just need to create an mbtiles file. The mbutil utility can do this. Install mbutil git clone http://github.com/mapbox/mbutil.git Change directory to the base of the Digital Globe tileset Sub-directories at this level should be numbers (e.g., /12/) Create metadata.json file: echo '{\"name\": \"<desired_tileset_name>\", \"format\": \"jpg\"}' > metadata.json Create the .mbtiles file: mbutil/mb-util --image_format=jpg ./ <filename>.mbtiles","title":"Digital Globe Imagery"},{"location":"maps.html#national-map-imagery-method-2_1","text":"In the folder containing the downloaded .jp2 files, run the following commands: gdalbuildvrt index.vrt *.jp2 gdalwarp -r near --config GDAL_CACHEMAX 3000 -wm 3000 -co compress=jpeg -co photometric=ycbcr -co tiled=yes -t_srs EPSG:3857 index.vrt <filename>.tif see http://www.gdal.org/gdalwarp.html for more options, particularly for the resampling method (-r flag) gdaladdo -r nearest --config COMPRESS_OVERVIEW JPEG --config PHOTOMETRIC_OVERVIEW YCBCR <filename>.tif 2 4 8 16 32 64 128 see http://www.gdal.org/gdaladdo.html for more options, particularly for the resampling method (-r flag) Use gdal2tiles.py to cut the tiles. Install gdal2tiles.py sudo apt-get install python-gdal gdal2tiles.py <filename>.tif This will create a folder called with appropriate tiles in it Use mbutil to create the mbtiles file Install mbutil git clone http://github.com/mapbox/mbutil.git ./mbutil/mb-util --scheme=tms <filename>/ <filename>.mbtiles","title":"National Map Imagery (method 2)"},{"location":"point_styles.html","text":"Map / Timeline Marker Styles \u00b6 This section describes marker style definition and associating these styles with event_types and entity_types. These marker styles are used to represent event types and entity types on the map and in the event timeline. The icons are typically taken from the FontAwesome set or from locally-served .svg icons. Marker Styles \u00b6 Markers are styled using the PointStyle model in Mole. It has the following fields: name: Name of this point style description: Description of this point style icon: String representing the icon to be used from the FontAwesome 5 library. Find icons here . color: CSS color code to be applied to the icon (e.g., #FFFFFF for white) use_marker_pin: Boolean field indicating wheather this icon should be shown in a marker pin on the map. Note this has no effect on the event timeline. marker_color: CSS color code to be applied to the map marker pin icon (e.g., #FFFFFF for white) scale_factor: An optional parameter to adjust sizing of the icon. Marker Compositing \u00b6 Point style is an element (foreign key) of the EventType and EntityType models. In order to determine how a particular event marker is to be styled, the following method is used: The point style associated with the event's event_type is used as a starting point. The metadata_style_fields (on event_type) is used to override this style. It represents a list of fields (keys) within the event's metadata whose values represent the name of an entity. For each entity in this list, the entity's entity_type point_style replaces (for any non-null values) the associated event's event_type point_style in order. In this way, fields occuring at the end of the metadata_style_fields have precedence over earlier occuring fields. Note: if only a single point_style element is desired to be replaced by the entity_type style (e.g. the icon string), all other fields within its point_style may be null. The resulting point_style is returned in the event's point_style field.","title":"Point Styles"},{"location":"point_styles.html#map-timeline-marker-styles","text":"This section describes marker style definition and associating these styles with event_types and entity_types. These marker styles are used to represent event types and entity types on the map and in the event timeline. The icons are typically taken from the FontAwesome set or from locally-served .svg icons.","title":"Map / Timeline Marker Styles"},{"location":"point_styles.html#marker-styles","text":"Markers are styled using the PointStyle model in Mole. It has the following fields: name: Name of this point style description: Description of this point style icon: String representing the icon to be used from the FontAwesome 5 library. Find icons here . color: CSS color code to be applied to the icon (e.g., #FFFFFF for white) use_marker_pin: Boolean field indicating wheather this icon should be shown in a marker pin on the map. Note this has no effect on the event timeline. marker_color: CSS color code to be applied to the map marker pin icon (e.g., #FFFFFF for white) scale_factor: An optional parameter to adjust sizing of the icon.","title":"Marker Styles"},{"location":"point_styles.html#marker-compositing","text":"Point style is an element (foreign key) of the EventType and EntityType models. In order to determine how a particular event marker is to be styled, the following method is used: The point style associated with the event's event_type is used as a starting point. The metadata_style_fields (on event_type) is used to override this style. It represents a list of fields (keys) within the event's metadata whose values represent the name of an entity. For each entity in this list, the entity's entity_type point_style replaces (for any non-null values) the associated event's event_type point_style in order. In this way, fields occuring at the end of the metadata_style_fields have precedence over earlier occuring fields. Note: if only a single point_style element is desired to be replaced by the entity_type style (e.g. the icon string), all other fields within its point_style may be null. The resulting point_style is returned in the event's point_style field.","title":"Marker Compositing"},{"location":"pulsar.html","text":"Apache Pulsar \u00b6 Overview \u00b6 Apache Pulsar is a distributed messaging framework. This is the main messaging system used for Mole. Pulsar also has support for stream processing through their service (Pulsar Functions), which we have the ability of leveraging for our use cases. Architecture overview We will be running the standalone mode which contains all the pieces of a Pulsar instance in a single Docker image. Configuration \u00b6 Configuring Pulsar Functions: Pulsar doesn't start with any Pulsar functions running. In order to run them, they need to be created in the Pulsar broker. There are three ways to deploy these functions. A Java client interface the pulsar-admin CLI tool HTTP calls to the admin REST API Of these, we will mainly be using HTTP calls and the pulsar-admin tool. There is a python script that will automatically post any available Pulsar functions it finds on the creation of a new Pulsar Docker container. This script, pulsar/create_functions.py , searches the functions directory for python files, assuming any python files in them to be valid Pulsar functions files and skipping over files prepended an _ , and crafts a HTTP payload from a python dict within each Pulsar function file and posts it to the admin REST API. We will only be using the Pulsar Function SDK for Python to develop these functions. This SDK provides a bigger range of functionality that is not available if we were to use the language-native interface. Each Pulsar function file needs at minimum the following: a dict defining the configuration of the Pulsar function a class that inherits from pulsar.Function that defines a process method with necessary parameters Fields: \u00b6 The configuration of the Pulsar function has a number of required fields: py \u00b6 The location of the python file. Note that this is the path within the docker container. Since the files will be located at /pulsar/functions/ , it will need to be of the form /pulsar/functions/<file> . \"py\": \"/pulsar/functions/example_function.py\" className \u00b6 The name of the class for the pulsar function. For python, the filename is necessary but not included in the class name. The class name will be used as the default name of the function and corresponding fully qualified function name if one is not specified. The classname in this example would be Example . \"className\": \"example_function.Example\" inputs \u00b6 This is a list of input topics for this function to monitor. The list can span multiple tenants, namespaces, and topics. However, if a regex pattern is used, all topics matching the pattern must be in the same tenant and namespace. \"inputs\": [\"persistent://public/default/input1\"] There are also a number of optional fields that can be specified if necessary. output \u00b6 The output topic that any return values will be sent to. If this isn't set manually, the output topic has a default value of {input topic}-{function name}-output \"output\": \"persistent://public/default/output\" logTopic \u00b6 This is the topic where logs for this Pulsar function are sent. \"logTopic\": \"persistent://public/default/log_topic\" fqfn \u00b6 This is the fully qualified function name (tenant/namespace/function_name). If not explicitly set, it will be inferred from the input topics and class name. \"fqfn\": \"public/default/example_function\" Example configuration \u00b6 body = { \"fqfn\": \"public/default/example_function\", \"py\": \"/pulsar/functions/example_function.py\", \"className\": \"example_function.Example\", \"inputs\": [\"persistent://public/default/input1\"], \"output\": \"persistent://public/default/output\", \"logTopic\": \"persistent://public/default/log_topic\", } Developing Pulsar Functions: \u00b6 Each Pulsar function needs to inherit from pulsar.Function . This gives us access to a context object that lets us have more functionality. Each function will need a process method. Function.process(self, input, context) input will be the message in bytes. context will provide a number of useful functions. (https://pulsar.apache.org/docs/en/functions-develop/#context) This process method will be called for each message that comes in on the list of input topics. Any return values from this method will be sent to the output topic. Logs can be sent by using the get_logger method to get a logger object and calling the corresponding method ( info , debug , warn , error , critical ) on it. The messages created by these log methods are sent to the log topic, where any consumers to that topic can display it. Note The state storage functions ( put_state , get_state , incr_counter , get_counter , del_counter ) aren't available because the state service is disabled. Use Redis instead. Note If using regex input topics for Pulsar functions, it is possible for the initial messages to not be captured. When a new topic that matches the regex is created by a producer, the Pulsar function has to then create a subscription to that topic which may take up to a minute. In that timespan, any messages on that topic will be missed and not processed by the Pulsar function. Link to Pulsar issue Testing Pulsar Functions: \u00b6 Unit testing the Pulsar functions can be done by running a custom docker-compose file. docker-compose -f docker-compose-tests.yml up This will run any tests in the pulsar/tests directory. Websockets \u00b6 Pulsar provides a websocket server for clients that do not have a Pulsar library. Clients can connect to this websocket server to receive and send messages on Pulsar topics. Note that the idle timeout for the websocket is 5 minutes . If expected to be idle for more than 5 minutes, the client should expect to reconnect when the websocket closes or send periodic messages to keep the websocket open.","title":"Pulsar Functions"},{"location":"pulsar.html#apache-pulsar","text":"","title":"Apache Pulsar"},{"location":"pulsar.html#overview","text":"Apache Pulsar is a distributed messaging framework. This is the main messaging system used for Mole. Pulsar also has support for stream processing through their service (Pulsar Functions), which we have the ability of leveraging for our use cases. Architecture overview We will be running the standalone mode which contains all the pieces of a Pulsar instance in a single Docker image.","title":"Overview"},{"location":"pulsar.html#configuration","text":"Configuring Pulsar Functions: Pulsar doesn't start with any Pulsar functions running. In order to run them, they need to be created in the Pulsar broker. There are three ways to deploy these functions. A Java client interface the pulsar-admin CLI tool HTTP calls to the admin REST API Of these, we will mainly be using HTTP calls and the pulsar-admin tool. There is a python script that will automatically post any available Pulsar functions it finds on the creation of a new Pulsar Docker container. This script, pulsar/create_functions.py , searches the functions directory for python files, assuming any python files in them to be valid Pulsar functions files and skipping over files prepended an _ , and crafts a HTTP payload from a python dict within each Pulsar function file and posts it to the admin REST API. We will only be using the Pulsar Function SDK for Python to develop these functions. This SDK provides a bigger range of functionality that is not available if we were to use the language-native interface. Each Pulsar function file needs at minimum the following: a dict defining the configuration of the Pulsar function a class that inherits from pulsar.Function that defines a process method with necessary parameters","title":"Configuration"},{"location":"pulsar.html#fields","text":"The configuration of the Pulsar function has a number of required fields:","title":"Fields:"},{"location":"pulsar.html#py","text":"The location of the python file. Note that this is the path within the docker container. Since the files will be located at /pulsar/functions/ , it will need to be of the form /pulsar/functions/<file> . \"py\": \"/pulsar/functions/example_function.py\"","title":"py"},{"location":"pulsar.html#classname","text":"The name of the class for the pulsar function. For python, the filename is necessary but not included in the class name. The class name will be used as the default name of the function and corresponding fully qualified function name if one is not specified. The classname in this example would be Example . \"className\": \"example_function.Example\"","title":"className"},{"location":"pulsar.html#inputs","text":"This is a list of input topics for this function to monitor. The list can span multiple tenants, namespaces, and topics. However, if a regex pattern is used, all topics matching the pattern must be in the same tenant and namespace. \"inputs\": [\"persistent://public/default/input1\"] There are also a number of optional fields that can be specified if necessary.","title":"inputs"},{"location":"pulsar.html#output","text":"The output topic that any return values will be sent to. If this isn't set manually, the output topic has a default value of {input topic}-{function name}-output \"output\": \"persistent://public/default/output\"","title":"output"},{"location":"pulsar.html#logtopic","text":"This is the topic where logs for this Pulsar function are sent. \"logTopic\": \"persistent://public/default/log_topic\"","title":"logTopic"},{"location":"pulsar.html#fqfn","text":"This is the fully qualified function name (tenant/namespace/function_name). If not explicitly set, it will be inferred from the input topics and class name. \"fqfn\": \"public/default/example_function\"","title":"fqfn"},{"location":"pulsar.html#example-configuration","text":"body = { \"fqfn\": \"public/default/example_function\", \"py\": \"/pulsar/functions/example_function.py\", \"className\": \"example_function.Example\", \"inputs\": [\"persistent://public/default/input1\"], \"output\": \"persistent://public/default/output\", \"logTopic\": \"persistent://public/default/log_topic\", }","title":"Example configuration"},{"location":"pulsar.html#developing-pulsar-functions","text":"Each Pulsar function needs to inherit from pulsar.Function . This gives us access to a context object that lets us have more functionality. Each function will need a process method. Function.process(self, input, context) input will be the message in bytes. context will provide a number of useful functions. (https://pulsar.apache.org/docs/en/functions-develop/#context) This process method will be called for each message that comes in on the list of input topics. Any return values from this method will be sent to the output topic. Logs can be sent by using the get_logger method to get a logger object and calling the corresponding method ( info , debug , warn , error , critical ) on it. The messages created by these log methods are sent to the log topic, where any consumers to that topic can display it. Note The state storage functions ( put_state , get_state , incr_counter , get_counter , del_counter ) aren't available because the state service is disabled. Use Redis instead. Note If using regex input topics for Pulsar functions, it is possible for the initial messages to not be captured. When a new topic that matches the regex is created by a producer, the Pulsar function has to then create a subscription to that topic which may take up to a minute. In that timespan, any messages on that topic will be missed and not processed by the Pulsar function. Link to Pulsar issue","title":"Developing Pulsar Functions:"},{"location":"pulsar.html#testing-pulsar-functions","text":"Unit testing the Pulsar functions can be done by running a custom docker-compose file. docker-compose -f docker-compose-tests.yml up This will run any tests in the pulsar/tests directory.","title":"Testing Pulsar Functions:"},{"location":"pulsar.html#websockets","text":"Pulsar provides a websocket server for clients that do not have a Pulsar library. Clients can connect to this websocket server to receive and send messages on Pulsar topics. Note that the idle timeout for the websocket is 5 minutes . If expected to be idle for more than 5 minutes, the client should expect to reconnect when the websocket closes or send periodic messages to keep the websocket open.","title":"Websockets"},{"location":"report_generator.html","text":"Report Generator \u00b6 The Report Generator is web application that utilizes the Plotly Dash Python Library to generate, print, and export meaningful data visualizations. The Report Generator is ideal for: Generating data visualization to further analyze collected data with Mole. Generating data visualization and analyze output as a standalone Dash app via API endpoint or CSV upload. About the App \u00b6 This is an interactive, multi-page report that dynamically creates figures with an API endpoint data and/or CSV data. The report also incorporates custom styling to provide distinct pages for PDF printing and the ability to export figures to static image file formats like PNG, SVG, or PDF. Getting Started \u00b6 Navigate to the root directory of the project and run the following command: $ ./ml init The Report Generator is available at http://localhost:8400 or http://localhost/report . Tip If Mole is running on separate machine, you can run the report as a standalone app using ./ml report . The report generator has an input box for users to enter an API endpoint manually. Quickstart \u00b6 Quickly get started on creating Plotly figures with the report generator. Create a graph function \u00b6 Navigate to /data/graph_functions.py and create a function with the following parameters: For figures using endpoint data, the function name must start with or contain api_ . def api_ < name_of_your_figure > ( endpoint , trial , font_color , plot_color , height , width ): # Perform endpoint request and create plotly figure. title = \"Name of Your Figure\" if endpoint : data = qf . api_request ( f \" { endpoint } /event_data?trial= { trial } \" ) else : # If an endpoint is not provided on the UI, a default endpoint # (Mole event data endpoint) will be used. data = qf . api_request ( f \" { event_endpoint } ?trial= { trial } \" ) # Create a figure with the endpoint data. # Figure placeholder fig = empty_figure ( font_color , plot_color , height , width , title ) return fig , \"name_of_your_figure\" For figures using csv data, the function name must start with or contain csv_ . def csv_ < name_of_your_figure > ( data , filename , font_color , plot_color , height , width ): # Create plotly figure. The uploaded data is accessible through parameter 'data'. fig = go . Figure () # Create a figure with the csv data. return fig , \"name_of_your_figure\" Displaying the figure \u00b6 By default, the report generator comes with two pages (tabs), api and csv . To display a figure, navigate to /pages and select the respective page. For example, to display a figure on the api page: # /pages/api.py def metric_layout (): \"\"\"Return graph figures that uses api data endpoints.\"\"\" return html . Div ( [ rm . create_figure ( id = \"api_<name_of_your_figure\" ), rm . create_figure ( id = \"api_event_bar\" ), rm . create_figure ( id = \"api_event_timeline\" ), ], className = \"row \" , style = { \"marginTop\" : \"5px\" }, ) Restart the report container and refresh the page to see the newly created figure. Developer Guide \u00b6 Inside the report_generator directory, you'll find app.py . This is where the Report Generator (Dash app) is being executed. Building the Report \u00b6 The /dash directory contains the core components of the report. base.py is the base class of the Dash app. The dynamic callbacks functionality are defined in this file. report_modules.py consists of reusable report UI components such as dcc.Graph and dcc.Dropdown , or a custom Div such as html.Div() . components.py contains all custom UI callback components. All data querying, analysis, and visualization are located in /data directory. graph_figures.py contains functions returning plotly figures. query_functions.py contains a reusable endpoint request function that covert endpoint data to a DataFrame . All data manipulation and analysis should be done in this file as well. utils.py contains other useful data functions such as parsing CSV file. Each tab on the Report Generator has its own file in the /pages directory. The layout.py contains the functional code to load and display all of the contents for the app. Creating Figures via API Endpoint \u00b6 To retrieve data from an API endpoint, the following function is used: # /data/query_functions.py def api_request ( source ): \"\"\"Make an API GET-request and return endpoint data as a dataframe.\"\"\" r = requests . get ( source ) if r . status_code == 200 : data = r . content endpoint_data = pd . read_csv ( io . StringIO ( data . decode ( 'utf-8' ))) else : endpoint_data = pd . DataFrame () return endpoint_data The parameter source is the endpoint. To get Mole event data, the source is http://django:8000/api/event_data . Note The default configurations for request is set in config.yml . You can also add filters to the endpoint. For example, to filter event data by their trial_id : source = \"http://django:8000/api/event_data?trial=1\" is equivalent to: source = f\"{event_endpoint}?trial=1\" The event_endpoint is a variable set after reading the config.yml . You can also download the endpoint data by inputting the url in your browser. More examples on Mole endpoints are found in /data/csv/mole_endpoint_examples.csv . The report generator also display these examples by default on a table figure that is found under the CSV Upload tab. To create a figure using the endpoint data, the figure function name in graph_functions.py must start with api_ : def api_event_bar ( endpoint , trial , font_color , plot_color , height , width ): fig = Go . Figure () return figure , \"name_of_figure\" The callback function for this figure will be created dynamically. This dynamically generated callback has the following inputs: interval-component , fires the callback periodically based on the given polling interval set in layout.py url , the current page based on the URL. Switching between Report and Dashboard will trigger the callback. endpoint-session , the string returned by the endpoint input box used for manually entering an API endpoint. This string is assigned to the endpoint paramter. trial-selector , the value returned by the trial selector used for filtering event endpoint data. This value is assigned to the trial parameter. The output of the callback is figure which is a property of the Graph . Whenever there's a change in the input , Dash calls the callback function constructs a figure object from graph_functions , and returns it to the app. To display this dynamically generated figure on the report, use report_modules.create_figures() on the desired page layout. # /pages/api.py def metric_layout (): \"\"\"Return graph figures that uses api data endpoints.\"\"\" return html . Div ( [ rm . create_figure ( id = \"api_event_bar\" ), ], className = \"row \" , style = { \"marginTop\" : \"5px\" }, ) The id is the name of the figure function. In this example, the id is api_event_bar . Tip The Report Generator comes with two sample figures. You can generate random events to Mole by clicking the GENERATE EVENT button after entering Mole credentials on the report generator dashboard. NOTE : The events generated will be posted to the current trial on Mole, not the selected trial on the report generator. Creating Figures via CSV \u00b6 By default, the app will automatically upload the mole_endpoint_examples.csv from /data/csv . This can be changed by modifying the DEFAULT_DATA in utils.py . There is also an option to upload your own file from the report dashboard. To dynamically create a figure with uploaded data, the figure function name in graph_functions.py must start with csv_ : def csv_table ( data , filename , font_color , plot_color , height , width ): fig = go . Figure () return fig , \"export_filename\" The dynamic callback for this CSV-figure has the following inputs: url , the current page based on the URL. Switching between Print and Dashboard will trigger the callback. csv-session , the data acquired from upload. csv-filename-session , the filename of the uploaded data. The output of the callback is figure which is a property of the Graph . Whenever there's a change in the input , Dash calls the callback function constructs a figure object from graph_functions , and returns it to the app. Exporting Figures \u00b6 To export figures from the Report Generator, simply click the Export button on the dashboard. Every figure in the dashboard can be exported as long the function in graph_functions.py has the export decorator from export.py . To use the export decorator , create the decorator: import report_generator.data.export as re bulk_export = re . make_bulk_exported () Then for any functions that create a figure, ensure that it can accept any number of parameters using args and kwargs . All figure functions with the same export call will be passed with the same set of parameters. Ensure all functions in graph_functions.py with export decorator to return a figure and title. By default, there are two decorators in place: bulk_export , this export decorator is used for exporting figures that uses endpoint data. It will export every figure for every trial. export , this export decorator is used for exporting figures that uses CSV data. # /data/graph_functions.py @bulk_export def example ( trial , font_color , plot_color , height , width ): fig = go . Figure () title = \"Example\" fig . update_layout ( title = title ) return fig , f \"example_ { trial } \" All exported figures can be found in report/exported_figures . PDF Print/Export \u00b6 The Report Generator comes with a dedicated page that is styled specfically for PDF exporting. This page can be found at http://localhost/report/Print . To export the report to a PDF, simply click the export button on the top right corner of the page.","title":"Report Generator"},{"location":"report_generator.html#report-generator","text":"The Report Generator is web application that utilizes the Plotly Dash Python Library to generate, print, and export meaningful data visualizations. The Report Generator is ideal for: Generating data visualization to further analyze collected data with Mole. Generating data visualization and analyze output as a standalone Dash app via API endpoint or CSV upload.","title":"Report Generator"},{"location":"report_generator.html#about-the-app","text":"This is an interactive, multi-page report that dynamically creates figures with an API endpoint data and/or CSV data. The report also incorporates custom styling to provide distinct pages for PDF printing and the ability to export figures to static image file formats like PNG, SVG, or PDF.","title":"About the App"},{"location":"report_generator.html#getting-started","text":"Navigate to the root directory of the project and run the following command: $ ./ml init The Report Generator is available at http://localhost:8400 or http://localhost/report . Tip If Mole is running on separate machine, you can run the report as a standalone app using ./ml report . The report generator has an input box for users to enter an API endpoint manually.","title":"Getting Started"},{"location":"report_generator.html#quickstart","text":"Quickly get started on creating Plotly figures with the report generator.","title":"Quickstart"},{"location":"report_generator.html#create-a-graph-function","text":"Navigate to /data/graph_functions.py and create a function with the following parameters: For figures using endpoint data, the function name must start with or contain api_ . def api_ < name_of_your_figure > ( endpoint , trial , font_color , plot_color , height , width ): # Perform endpoint request and create plotly figure. title = \"Name of Your Figure\" if endpoint : data = qf . api_request ( f \" { endpoint } /event_data?trial= { trial } \" ) else : # If an endpoint is not provided on the UI, a default endpoint # (Mole event data endpoint) will be used. data = qf . api_request ( f \" { event_endpoint } ?trial= { trial } \" ) # Create a figure with the endpoint data. # Figure placeholder fig = empty_figure ( font_color , plot_color , height , width , title ) return fig , \"name_of_your_figure\" For figures using csv data, the function name must start with or contain csv_ . def csv_ < name_of_your_figure > ( data , filename , font_color , plot_color , height , width ): # Create plotly figure. The uploaded data is accessible through parameter 'data'. fig = go . Figure () # Create a figure with the csv data. return fig , \"name_of_your_figure\"","title":"Create a graph function"},{"location":"report_generator.html#displaying-the-figure","text":"By default, the report generator comes with two pages (tabs), api and csv . To display a figure, navigate to /pages and select the respective page. For example, to display a figure on the api page: # /pages/api.py def metric_layout (): \"\"\"Return graph figures that uses api data endpoints.\"\"\" return html . Div ( [ rm . create_figure ( id = \"api_<name_of_your_figure\" ), rm . create_figure ( id = \"api_event_bar\" ), rm . create_figure ( id = \"api_event_timeline\" ), ], className = \"row \" , style = { \"marginTop\" : \"5px\" }, ) Restart the report container and refresh the page to see the newly created figure.","title":"Displaying the figure"},{"location":"report_generator.html#developer-guide","text":"Inside the report_generator directory, you'll find app.py . This is where the Report Generator (Dash app) is being executed.","title":"Developer Guide"},{"location":"report_generator.html#building-the-report","text":"The /dash directory contains the core components of the report. base.py is the base class of the Dash app. The dynamic callbacks functionality are defined in this file. report_modules.py consists of reusable report UI components such as dcc.Graph and dcc.Dropdown , or a custom Div such as html.Div() . components.py contains all custom UI callback components. All data querying, analysis, and visualization are located in /data directory. graph_figures.py contains functions returning plotly figures. query_functions.py contains a reusable endpoint request function that covert endpoint data to a DataFrame . All data manipulation and analysis should be done in this file as well. utils.py contains other useful data functions such as parsing CSV file. Each tab on the Report Generator has its own file in the /pages directory. The layout.py contains the functional code to load and display all of the contents for the app.","title":"Building the Report"},{"location":"report_generator.html#creating-figures-via-api-endpoint","text":"To retrieve data from an API endpoint, the following function is used: # /data/query_functions.py def api_request ( source ): \"\"\"Make an API GET-request and return endpoint data as a dataframe.\"\"\" r = requests . get ( source ) if r . status_code == 200 : data = r . content endpoint_data = pd . read_csv ( io . StringIO ( data . decode ( 'utf-8' ))) else : endpoint_data = pd . DataFrame () return endpoint_data The parameter source is the endpoint. To get Mole event data, the source is http://django:8000/api/event_data . Note The default configurations for request is set in config.yml . You can also add filters to the endpoint. For example, to filter event data by their trial_id : source = \"http://django:8000/api/event_data?trial=1\" is equivalent to: source = f\"{event_endpoint}?trial=1\" The event_endpoint is a variable set after reading the config.yml . You can also download the endpoint data by inputting the url in your browser. More examples on Mole endpoints are found in /data/csv/mole_endpoint_examples.csv . The report generator also display these examples by default on a table figure that is found under the CSV Upload tab. To create a figure using the endpoint data, the figure function name in graph_functions.py must start with api_ : def api_event_bar ( endpoint , trial , font_color , plot_color , height , width ): fig = Go . Figure () return figure , \"name_of_figure\" The callback function for this figure will be created dynamically. This dynamically generated callback has the following inputs: interval-component , fires the callback periodically based on the given polling interval set in layout.py url , the current page based on the URL. Switching between Report and Dashboard will trigger the callback. endpoint-session , the string returned by the endpoint input box used for manually entering an API endpoint. This string is assigned to the endpoint paramter. trial-selector , the value returned by the trial selector used for filtering event endpoint data. This value is assigned to the trial parameter. The output of the callback is figure which is a property of the Graph . Whenever there's a change in the input , Dash calls the callback function constructs a figure object from graph_functions , and returns it to the app. To display this dynamically generated figure on the report, use report_modules.create_figures() on the desired page layout. # /pages/api.py def metric_layout (): \"\"\"Return graph figures that uses api data endpoints.\"\"\" return html . Div ( [ rm . create_figure ( id = \"api_event_bar\" ), ], className = \"row \" , style = { \"marginTop\" : \"5px\" }, ) The id is the name of the figure function. In this example, the id is api_event_bar . Tip The Report Generator comes with two sample figures. You can generate random events to Mole by clicking the GENERATE EVENT button after entering Mole credentials on the report generator dashboard. NOTE : The events generated will be posted to the current trial on Mole, not the selected trial on the report generator.","title":"Creating Figures via API Endpoint"},{"location":"report_generator.html#creating-figures-via-csv","text":"By default, the app will automatically upload the mole_endpoint_examples.csv from /data/csv . This can be changed by modifying the DEFAULT_DATA in utils.py . There is also an option to upload your own file from the report dashboard. To dynamically create a figure with uploaded data, the figure function name in graph_functions.py must start with csv_ : def csv_table ( data , filename , font_color , plot_color , height , width ): fig = go . Figure () return fig , \"export_filename\" The dynamic callback for this CSV-figure has the following inputs: url , the current page based on the URL. Switching between Print and Dashboard will trigger the callback. csv-session , the data acquired from upload. csv-filename-session , the filename of the uploaded data. The output of the callback is figure which is a property of the Graph . Whenever there's a change in the input , Dash calls the callback function constructs a figure object from graph_functions , and returns it to the app.","title":"Creating Figures via CSV"},{"location":"report_generator.html#exporting-figures","text":"To export figures from the Report Generator, simply click the Export button on the dashboard. Every figure in the dashboard can be exported as long the function in graph_functions.py has the export decorator from export.py . To use the export decorator , create the decorator: import report_generator.data.export as re bulk_export = re . make_bulk_exported () Then for any functions that create a figure, ensure that it can accept any number of parameters using args and kwargs . All figure functions with the same export call will be passed with the same set of parameters. Ensure all functions in graph_functions.py with export decorator to return a figure and title. By default, there are two decorators in place: bulk_export , this export decorator is used for exporting figures that uses endpoint data. It will export every figure for every trial. export , this export decorator is used for exporting figures that uses CSV data. # /data/graph_functions.py @bulk_export def example ( trial , font_color , plot_color , height , width ): fig = go . Figure () title = \"Example\" fig . update_layout ( title = title ) return fig , f \"example_ { trial } \" All exported figures can be found in report/exported_figures .","title":"Exporting Figures"},{"location":"report_generator.html#pdf-printexport","text":"The Report Generator comes with a dedicated page that is styled specfically for PDF exporting. This page can be found at http://localhost/report/Print . To export the report to a PDF, simply click the export button on the top right corner of the page.","title":"PDF Print/Export"},{"location":"scenario_scripts.html","text":"Description \u00b6 Scenario Scripts allow you to describe a sequence of events to occur after a certain event occurs and trial Conditions are met. Scripted Events are scheduled to post after a defined amount of time from either the initiating event or the Scripted Event that precedes it. Configuration \u00b6 Step 1: Describe the Script \u00b6 A Script describes: initiating_event_types : a list of event types that can initiate the Script conditions : optional Conditions that need to be met for the Script to run conditions_pass_if_any : When multiple conditions are used, will use OR logic instead of AND run_limit : an optional number of times the Script is allowed to run in a given trial auto_repeat_count : an optional number of times to repeat the script when it's triggered cancelling_event_type : the event type that can cancel scheduled events scripted_event_head : the first Scripted Event in the Script sequence* Note *Scripts are composed of a linked list of Scripted events, which we refer to as the Script sequence. Example: \u00b6 trial_start_script = auto_factories . ScriptFactory ( name = \"On Trial Start Script\" , initiating_event_types = [ trial_start_event_type ], conditions = [ has_trial_init_condition ], # See Step 3 run_limit = 5 , auto_repeat_count = 3 , cancelling_event_type = cancel_scheduled_events_event_type , scripted_event_head =... # See Step 2 ) Step 2: Describe the scheduled events \u00b6 Scripted Events describe: event_type : an event type to post delay_seconds : the time delay before posting the event conditions : Conditions to be met for the event to post conditions_pass_if_any : When multiple conditions are used, will use OR logic instead of AND add_event_metadata : metadata to attach to the event being created copy_trigger_metadata : copies metadata from triggering event to the event being created next_scripted_event : the next event to schedule Note Though Scripted events can be written in isolation, the linked*list nature would require you to write their constructors in reverse, so it's recommended to nest the scheduled events in the Script. Example: \u00b6 trial_start_script = auto_factories . ScriptFactory ( name = \"On Trial Start Script\" , initiating_event_types = [ trial_start_event_type ], conditions = [ has_trial_init_condition ], # See Step 3 cancelling_event_type = cancel_scheduled_events_event_type , scripted_event_head = auto_factories . ScriptedEventFactory ( # Nested ScriptedEvent name = \"Create other event after 0 seconds\" , conditions = [ has_trial_init_w_agent_condition ], # See step 3 delay_seconds = 0 , event_type = other_event_type , add_event_metadata = { \"note\" : \"Created by script.\" }, copy_trigger_metadata = True , next_scripted_event = auto_factories . ScriptedEventFactory ( # Nested ScriptedEvent name = \"Create unassigned event after 15 seconds\" , conditions = [ ugv_prox_in_condition , has_trial_init_condition ], # See step 3 conditions_pass_if_any = True , delay_seconds = 15 , event_type = unassigned_event_type , event_metadata = { \"note\" : \"Created by script.\" }, next_scripted_event = None ) ) ) Step 3: Describe Conditions \u00b6 Script Conditions can be applied to both the Script and Scripted events. The Condition will be checked by Mole, and if met, Mole will either allow the Script to be run or allow the event to be scheduled. All rules within a condition use an OR operation; if any rule is true, the Condition will be true. If you would like an AND condition, break rules out into separate Conditions. The following rules can be set: trial_has_event : evaluates to true if trial contains the specified event type trial_missing_event : evaluates to true if trial does not contain the specified event type event_metadata_contains : evaluates to true if event from trial_has_event contains the specified string event_metadata_excludes : evaluates to true if event from trial_has_event excludes the specified string trigger_metadata_contains : evaluates to true if the triggering event's metadata contains the specified string trigger_metadata_excludes : evaluates to true if the triggering event's metadata excludes the specified string Note The event_metadata_contains field only applies in conjunction with trial_has_event , in which the condition will only be met if the event defined in trial_has_event has metadata that contains the string described in event_metadata_contains . Example: \u00b6 has_trial_init_w_agent_condition = auto_factories . ScriptConditionFactory ( trial_has_event = trial_init_event_type , trial_missing_event = trial_end_event_type , event_metadata_contains = \"agent\" , event_metadata_excludes = \"bad_meta\" , trigger_metadata_contains = \"node\" , trigger_metadata_excludes = \"bad_meta\" , ) Step 4: Add Script to Scenario \u00b6 Scripts are associated with Scenarios, but tracked by Trials. This primarily applies to run limits, where the Script can be limited to a certain number of runs for the given Trial. Conditions are also applied in the scope of the given Trial. Example: \u00b6 scripted_scenario = auto_factories . ScenarioFactory ( name = \"Example Scripted Scenario\" , description = \"Example Scripted Scenario\" , location = camp_roberts_location , test_method = interactive_fiducial_method , scripts = [ trial_start_script , trial_end_script ] )","title":"Scenario Scripts"},{"location":"scenario_scripts.html#description","text":"Scenario Scripts allow you to describe a sequence of events to occur after a certain event occurs and trial Conditions are met. Scripted Events are scheduled to post after a defined amount of time from either the initiating event or the Scripted Event that precedes it.","title":"Description"},{"location":"scenario_scripts.html#configuration","text":"","title":"Configuration"},{"location":"scenario_scripts.html#step-1-describe-the-script","text":"A Script describes: initiating_event_types : a list of event types that can initiate the Script conditions : optional Conditions that need to be met for the Script to run conditions_pass_if_any : When multiple conditions are used, will use OR logic instead of AND run_limit : an optional number of times the Script is allowed to run in a given trial auto_repeat_count : an optional number of times to repeat the script when it's triggered cancelling_event_type : the event type that can cancel scheduled events scripted_event_head : the first Scripted Event in the Script sequence* Note *Scripts are composed of a linked list of Scripted events, which we refer to as the Script sequence.","title":"Step 1: Describe the Script"},{"location":"scenario_scripts.html#example","text":"trial_start_script = auto_factories . ScriptFactory ( name = \"On Trial Start Script\" , initiating_event_types = [ trial_start_event_type ], conditions = [ has_trial_init_condition ], # See Step 3 run_limit = 5 , auto_repeat_count = 3 , cancelling_event_type = cancel_scheduled_events_event_type , scripted_event_head =... # See Step 2 )","title":"Example:"},{"location":"scenario_scripts.html#step-2-describe-the-scheduled-events","text":"Scripted Events describe: event_type : an event type to post delay_seconds : the time delay before posting the event conditions : Conditions to be met for the event to post conditions_pass_if_any : When multiple conditions are used, will use OR logic instead of AND add_event_metadata : metadata to attach to the event being created copy_trigger_metadata : copies metadata from triggering event to the event being created next_scripted_event : the next event to schedule Note Though Scripted events can be written in isolation, the linked*list nature would require you to write their constructors in reverse, so it's recommended to nest the scheduled events in the Script.","title":"Step 2: Describe the scheduled events"},{"location":"scenario_scripts.html#example_1","text":"trial_start_script = auto_factories . ScriptFactory ( name = \"On Trial Start Script\" , initiating_event_types = [ trial_start_event_type ], conditions = [ has_trial_init_condition ], # See Step 3 cancelling_event_type = cancel_scheduled_events_event_type , scripted_event_head = auto_factories . ScriptedEventFactory ( # Nested ScriptedEvent name = \"Create other event after 0 seconds\" , conditions = [ has_trial_init_w_agent_condition ], # See step 3 delay_seconds = 0 , event_type = other_event_type , add_event_metadata = { \"note\" : \"Created by script.\" }, copy_trigger_metadata = True , next_scripted_event = auto_factories . ScriptedEventFactory ( # Nested ScriptedEvent name = \"Create unassigned event after 15 seconds\" , conditions = [ ugv_prox_in_condition , has_trial_init_condition ], # See step 3 conditions_pass_if_any = True , delay_seconds = 15 , event_type = unassigned_event_type , event_metadata = { \"note\" : \"Created by script.\" }, next_scripted_event = None ) ) )","title":"Example:"},{"location":"scenario_scripts.html#step-3-describe-conditions","text":"Script Conditions can be applied to both the Script and Scripted events. The Condition will be checked by Mole, and if met, Mole will either allow the Script to be run or allow the event to be scheduled. All rules within a condition use an OR operation; if any rule is true, the Condition will be true. If you would like an AND condition, break rules out into separate Conditions. The following rules can be set: trial_has_event : evaluates to true if trial contains the specified event type trial_missing_event : evaluates to true if trial does not contain the specified event type event_metadata_contains : evaluates to true if event from trial_has_event contains the specified string event_metadata_excludes : evaluates to true if event from trial_has_event excludes the specified string trigger_metadata_contains : evaluates to true if the triggering event's metadata contains the specified string trigger_metadata_excludes : evaluates to true if the triggering event's metadata excludes the specified string Note The event_metadata_contains field only applies in conjunction with trial_has_event , in which the condition will only be met if the event defined in trial_has_event has metadata that contains the string described in event_metadata_contains .","title":"Step 3: Describe Conditions"},{"location":"scenario_scripts.html#example_2","text":"has_trial_init_w_agent_condition = auto_factories . ScriptConditionFactory ( trial_has_event = trial_init_event_type , trial_missing_event = trial_end_event_type , event_metadata_contains = \"agent\" , event_metadata_excludes = \"bad_meta\" , trigger_metadata_contains = \"node\" , trigger_metadata_excludes = \"bad_meta\" , )","title":"Example:"},{"location":"scenario_scripts.html#step-4-add-script-to-scenario","text":"Scripts are associated with Scenarios, but tracked by Trials. This primarily applies to run limits, where the Script can be limited to a certain number of runs for the given Trial. Conditions are also applied in the scope of the given Trial.","title":"Step 4: Add Script to Scenario"},{"location":"scenario_scripts.html#example_3","text":"scripted_scenario = auto_factories . ScenarioFactory ( name = \"Example Scripted Scenario\" , description = \"Example Scripted Scenario\" , location = camp_roberts_location , test_method = interactive_fiducial_method , scripts = [ trial_start_script , trial_end_script ] )","title":"Example:"},{"location":"trigger_setup.html","text":"Event Generator Pulsar Function \u00b6 There is a Pulsar function that aims to implement a Python based event generator, contained in functions/_simple_event_gen.py . This function is aided by a Python application ( message_cacher.py ) that caches relevant incoming Pulsar messages into a Redis cache with timestamped storage and easy retrieval for the purposes of trigger evaluation. This python application will reside in its own separate Docker container. As an overview, a trigger can be established by creating a Trigger model instance. Evaluation of a trigger involves its condition and the condition variables used. The condition is a string that is evaluated in an Python-like interpreter. Note: The file that contains the Pulsar function is prepended by an underscore to stop the pulsar function auto-discovery in create_functions.py from creating the function. message_cacher.py will create this function once it can retrieve the input topics from the Django server. Trigger Setup \u00b6 To create a Trigger , you need to have some pre-requisites models first, starting with the ConditionVariable . The ConditionVariable is used during the Trigger condition evaluation, where it acts as a placeholder. ConditionVariable \u00b6 Django ORM Factory Boy import data_collection.models as dcm my_condition_var = dcm . ConditionVariable . objects . create ( name = \"node_state_var\" , description = \"node state (e.g. unconfigured / active)\" , variable = \"node_state_var : /node/status.state\" , ) from data_collection.factories import factories my_condition_var = factories . ConditionVariableFactory ( name = \"node_state_var\" , description = \"node state (e.g. unconfigured / active)\" , variable = \"node_state_var : /node/status.state\" , ) The variable field should be in a similar format to the example: condition_variable_name : /topic/name.field_name . condition_variable_name is the variable name used in the Trigger evaluation. The topic name will be converted to underscores (i.e. /topic/name -> _topic_name ) and used as part of a Pulsar topic name. The Pulsar tenant and namespace cannot be changed and are set to public and default respectively. The resulting full Pulsar topic in this example would be persistent://public/default/_node_status . field_name signifies which key to use when replacing the condition_variable_name . When a message is received, the message is expected to have key-value pairs, one of which should have a key indicated by the field_name part of the variable field. condition_variable_name is replaced with the value of that key-value pair and the condition on the Trigger is evaluated. The example in this case will use the value from a message on Pulsar topic persistent://public/default/_node_status with a key of state and replace it wherever node_state_var appears in the Trigger 's condition. RequestedData \u00b6 RequestedData is any other data that you wish to post to some API endpoint. Django ORM Factory Boy import data_collection.models as dcm my_requested_data = dcm . RequestedData . objects . create ( name = \"Node Subject\" , description = \"Get subject entity when node registers\" , destination_url = \"$EVENT$\" , payload = { \"subject_entity\" : \"/node/status.entity_name\" , \"pose_source\" : \"[http://localhost:8000/api/pose_sources/1/]\" , \"cached_timestamp\" : \"$TIME$\" , }, }, ) from data_collection.factories import factories my_requested_data = factories . RequestedDataFactory ( name = \"Node Subject\" , description = \"Get subject entity when node registers\" , destination_url = \"$EVENT$\" , payload = { \"subject_entity\" : \"/node/status.entity_name\" , \"pose_source\" : \"[http://localhost:8000/api/pose_sources/1/]\" , \"cached_timestamp\" : \"$TIME$\" , }, ) destination_url indicates where the data will be posted to. $EVENT$ is a special string that indicates that the newly created event should be the destination. Since the event url won't be known prior to creating it, the string acts as a workaround to attach relevant data to the event. Another possible destination is the poses API endpoint which will create a pose. payload is a Python dictionary where the values are taken from message fields, similar to the ConditionVariable 's variable field. If the value is enclosed in square brackets, that static data will be posted. If the value is $EVENT$ or $TIME$, it will be replaced with the associated event or a timezone-aware datetime of the current time respectively. In this example, the subject_entity 's value is the entity_name field on a message from the Pulsar topic persistent://public/default/_node_status . The value of pose_source would be the literal string \"http://localhost:8000/api/pose_sources/1/\" regardless of the message contents. The value of cached_timestamp would be the time that the message is cached into Redis by message_cacher.py . Trigger \u00b6 With these pieces in place, you can create a Trigger now. Django ORM Factory Boy import data_collection.models as dcm dcm . Trigger . objects . create ( name = \"Node Online Trigger\" , key = \"node_online\" , description = \"Create event when a node comes online.\" , is_active = True , creates_event = True , condition = 'node_state_var == \"online\"' , condition_variables = [ my_condition_var ], requested_dataset = [ my_requested_data ], event_type = node_online_event_type , ) from data_collection.factories import factories factories . TriggerFactory ( name = \"Node Online Trigger\" , key = \"node_online\" , description = \"Create event when a node comes online.\" , is_active = True , creates_event = True , condition = 'node_state_var == \"online\"' , condition_variables = [ my_condition_var ], requested_dataset = [ my_requested_data ], event_type = node_online_event_type , ) key is a unique string that identifies the Trigger . is_active is a flag that will tell the event generator whether or not to monitor the Trigger condition. The flag is currently only read on start-up. It won't update while the event generator is running. creates_event is a flag that indicates whether or not the Trigger evaluation creates an event. condition is a string that represents the Python-like expression that is evaluated as the condition for the Trigger . condition_variables is a list of ConditionVariables that is used in the condition . It is an error if the condition contains any variables that doesn't have a corresponding condition_variable . requested_dataset is a list of RequestedData that will be attached to their respective destinations if this Trigger evaluates True. event_type is the default event type for events created by the Trigger . The event type can be overridden if it is present in the message for the Trigger . This is not used if creates_event is False.","title":"Trigger Setup"},{"location":"trigger_setup.html#event-generator-pulsar-function","text":"There is a Pulsar function that aims to implement a Python based event generator, contained in functions/_simple_event_gen.py . This function is aided by a Python application ( message_cacher.py ) that caches relevant incoming Pulsar messages into a Redis cache with timestamped storage and easy retrieval for the purposes of trigger evaluation. This python application will reside in its own separate Docker container. As an overview, a trigger can be established by creating a Trigger model instance. Evaluation of a trigger involves its condition and the condition variables used. The condition is a string that is evaluated in an Python-like interpreter. Note: The file that contains the Pulsar function is prepended by an underscore to stop the pulsar function auto-discovery in create_functions.py from creating the function. message_cacher.py will create this function once it can retrieve the input topics from the Django server.","title":"Event Generator Pulsar Function"},{"location":"trigger_setup.html#trigger-setup","text":"To create a Trigger , you need to have some pre-requisites models first, starting with the ConditionVariable . The ConditionVariable is used during the Trigger condition evaluation, where it acts as a placeholder.","title":"Trigger Setup"},{"location":"trigger_setup.html#conditionvariable","text":"Django ORM Factory Boy import data_collection.models as dcm my_condition_var = dcm . ConditionVariable . objects . create ( name = \"node_state_var\" , description = \"node state (e.g. unconfigured / active)\" , variable = \"node_state_var : /node/status.state\" , ) from data_collection.factories import factories my_condition_var = factories . ConditionVariableFactory ( name = \"node_state_var\" , description = \"node state (e.g. unconfigured / active)\" , variable = \"node_state_var : /node/status.state\" , ) The variable field should be in a similar format to the example: condition_variable_name : /topic/name.field_name . condition_variable_name is the variable name used in the Trigger evaluation. The topic name will be converted to underscores (i.e. /topic/name -> _topic_name ) and used as part of a Pulsar topic name. The Pulsar tenant and namespace cannot be changed and are set to public and default respectively. The resulting full Pulsar topic in this example would be persistent://public/default/_node_status . field_name signifies which key to use when replacing the condition_variable_name . When a message is received, the message is expected to have key-value pairs, one of which should have a key indicated by the field_name part of the variable field. condition_variable_name is replaced with the value of that key-value pair and the condition on the Trigger is evaluated. The example in this case will use the value from a message on Pulsar topic persistent://public/default/_node_status with a key of state and replace it wherever node_state_var appears in the Trigger 's condition.","title":"ConditionVariable"},{"location":"trigger_setup.html#requesteddata","text":"RequestedData is any other data that you wish to post to some API endpoint. Django ORM Factory Boy import data_collection.models as dcm my_requested_data = dcm . RequestedData . objects . create ( name = \"Node Subject\" , description = \"Get subject entity when node registers\" , destination_url = \"$EVENT$\" , payload = { \"subject_entity\" : \"/node/status.entity_name\" , \"pose_source\" : \"[http://localhost:8000/api/pose_sources/1/]\" , \"cached_timestamp\" : \"$TIME$\" , }, }, ) from data_collection.factories import factories my_requested_data = factories . RequestedDataFactory ( name = \"Node Subject\" , description = \"Get subject entity when node registers\" , destination_url = \"$EVENT$\" , payload = { \"subject_entity\" : \"/node/status.entity_name\" , \"pose_source\" : \"[http://localhost:8000/api/pose_sources/1/]\" , \"cached_timestamp\" : \"$TIME$\" , }, ) destination_url indicates where the data will be posted to. $EVENT$ is a special string that indicates that the newly created event should be the destination. Since the event url won't be known prior to creating it, the string acts as a workaround to attach relevant data to the event. Another possible destination is the poses API endpoint which will create a pose. payload is a Python dictionary where the values are taken from message fields, similar to the ConditionVariable 's variable field. If the value is enclosed in square brackets, that static data will be posted. If the value is $EVENT$ or $TIME$, it will be replaced with the associated event or a timezone-aware datetime of the current time respectively. In this example, the subject_entity 's value is the entity_name field on a message from the Pulsar topic persistent://public/default/_node_status . The value of pose_source would be the literal string \"http://localhost:8000/api/pose_sources/1/\" regardless of the message contents. The value of cached_timestamp would be the time that the message is cached into Redis by message_cacher.py .","title":"RequestedData"},{"location":"trigger_setup.html#trigger","text":"With these pieces in place, you can create a Trigger now. Django ORM Factory Boy import data_collection.models as dcm dcm . Trigger . objects . create ( name = \"Node Online Trigger\" , key = \"node_online\" , description = \"Create event when a node comes online.\" , is_active = True , creates_event = True , condition = 'node_state_var == \"online\"' , condition_variables = [ my_condition_var ], requested_dataset = [ my_requested_data ], event_type = node_online_event_type , ) from data_collection.factories import factories factories . TriggerFactory ( name = \"Node Online Trigger\" , key = \"node_online\" , description = \"Create event when a node comes online.\" , is_active = True , creates_event = True , condition = 'node_state_var == \"online\"' , condition_variables = [ my_condition_var ], requested_dataset = [ my_requested_data ], event_type = node_online_event_type , ) key is a unique string that identifies the Trigger . is_active is a flag that will tell the event generator whether or not to monitor the Trigger condition. The flag is currently only read on start-up. It won't update while the event generator is running. creates_event is a flag that indicates whether or not the Trigger evaluation creates an event. condition is a string that represents the Python-like expression that is evaluated as the condition for the Trigger . condition_variables is a list of ConditionVariables that is used in the condition . It is an error if the condition contains any variables that doesn't have a corresponding condition_variable . requested_dataset is a list of RequestedData that will be attached to their respective destinations if this Trigger evaluates True. event_type is the default event type for events created by the Trigger . The event type can be overridden if it is present in the message for the Trigger . This is not used if creates_event is False.","title":"Trigger"},{"location":"usage.html","text":"API Usage \u00b6 For the API during an active test mission, we'll generally be concerned with creating one of the following: Poses , Events , Notes , Images . As opposed to the configuration which was largely done through the Django custom command, these will be created through their respective API endpoints. Poses \u00b6 Poses can be used to track entities during each test run. Here is an example of posting a pose. Minimal Comprehensive import datetime import requests pose_data = { \"lat\" : 32.67 , \"lon\" : - 117.24 , \"entity\" : \"/api/entities/alpha_1/\" , \"pose_source\" : \"/api/pose_sources/1/\" , \"timestamp\" : datetime . datetime . now () . isoformat (), \"trial\" : 1 , } requests . post ( \"http://localhost/api/poses/\" , json = pose_data , auth = ( \"admin\" , \"admin\" )) import datetime import requests pose_data = { \"lat\" : 32.67 , \"lon\" : - 117.24 , \"entity\" : \"/api/entities/alpha_1/\" , \"pose_source\" : \"/api/pose_sources/1/\" , \"timestamp\" : datetime . datetime . now () . isoformat (), \"trial\" : 1 , \"elevation\" : 0.0 , \"heading\" : 23.0 , \"speed\" : 23 , \"velocity\" : [ 133.0 , 2.0 ], } requests . post ( \"http://localhost/api/poses/\" , json = pose_data , auth = ( \"admin\" , \"admin\" )) entity is the entity that this pose is for. pose_source indicates what type of pose it is. This string should contain the id of a pose source. timestamp represents the datetime at which this pose happened. trial indicates which specific test run you want to associate this pose with. The same entity can have different paths through the test environment so this isolates the poses to their respective trial. This field would be assigned the trial id. elevation is an optional arbitrary float value. This could be used to represent meters from sea level or levels in a building. heading is an optional arbitrary float value. A good rule of thumb is to use 0.0 and/or 360.0 as north and go clockwise (i.e. east would be 90.0 ) speed and velocity are both optional fields. velocity is an array, allowing to provide either 2D or 3D vectors. speed isn't automatically calculated so you'll have to manually set the scalar value. There is no validation that the speed field is the correct speed for the velocity field. Events \u00b6 Events will be the primary mode of recording data. These will use the EventTypes created during the configuration and can include any other specific data and/or metadata that you wish to attach. The following is an example of posting an event. Minimal Comprehensive import datetime import requests json_to_post = { \"start_datetime\" : datetime . datetime . now () . isoformat (), \"event_type\" : \"/api/event_types/1/\" , \"trial\" : \"/api/trials/1/\" , } requests . post ( \"http://<Mole_IP>/api/events/\" , json = json_to_post , auth = ( \"admin\" , \"admin\" )) import datetime import requests pose_data = { \"lat\" : 32.67 , \"lon\" : - 117.24 , \"entity\" : \"/api/entities/alpha_1/\" , \"pose_source\" : \"/api/pose_sources/1/\" , \"timestamp\" : datetime . datetime . now () . isoformat (), \"trial\" : 1 , } pose_post = requests . post ( \"http://localhost/api/poses/\" , json = pose_data , auth = ( \"admin\" , \"admin\" )) pose_url = pose_post . headers [ 'Location' ] json_to_post = { \"start_datetime\" : datetime . datetime . now () . isoformat (), \"end_datetime\" : \"2022-01-01T08:00:00+00:00\" , \"event_type\" : \"/api/event_types/1/\" , \"trial\" : \"/api/trials/1/\" , \"start_pose\" : pose_url , \"metadata\" : { \"entity\" : \"usv1\" , \"extra_info\" : 25.2 , \"list_of_important_things\" : [ \"item1\" , \"item2\" , ], } } requests . post ( \"http://<Mole_IP>/api/events/\" , json = json_to_post , auth = ( \"admin\" , \"admin\" )) start_datetime and end_datetime should be a string in the format of ISO 8601. start_datetime is a required field. end_datetime is optional and intended for events with a duration of some kind. It can be omitted and added later or omitted entirely. event_type should be a string with the corresponding id for the event type you want to create. trial should be a string with the corresponding id for the trial you want to attach this event to. start_pose is an optional field that ties a pose to this event. Its value should be the url of a previously created pose. Currently, it is not possible to create both a pose and event in the same HTTP request. You can use this start pose field to indicate something like an interaction happening at a specific location. metadata should be a Python dictionary of relevent data and metadata about the event. If not supplied, it will default to an empty dictionary. The target url should be whatever IP the Mole server is hosted on. This could be an IP or http://localhost if posting locally from the same host. Note The crendentials used are based off the example configuration in the basic configuration . If these have changed, make sure to use the correct username and password. Notes \u00b6 Notes provide an area to supplement events with free-form notes. All notes must be attached to an event. import datetime import requests json_to_post = { \"tester\" : \"/api/testers/1/\" , \"note\" : \"Test note here\" , \"event\" : \"/api/events/1/\" , } requests . post ( \"http://localhost/api/notes/\" , json = json_to_post , auth = ( \"admin\" , \"admin\" )) Images \u00b6 Images can be useful for traceability and debugging. You can attach an image(s) to any event. import datetime import requests files = { \"image\" : open ( '/directory/to/image.png' , 'rb' ), } data = { \"image_type\" : \"/api/image_types/1/\" , \"event\" : \"/api/events/1/\" , \"timestamp\" : datetime . datetime . now () . isoformat (), } r = requests . post ( \"http://localhost/api/images/\" , files = files , data = data , auth = ( \"admin\" , \"admin\" )) Bulk POSTs \u00b6 Mole also has the ability to create multiple instances for a single HTTP POST request. Currently this ability only exists for events and poses. To use this ability, simply pass a list of objects rather than a single object. This feature is useful if single posts are too slow for the level of throughput desired and increasing the number of Gunicorn/Django workers is not possible. Testing might be required to determine the optimal number of objects to post during each request. Also note that Pulsar messages will not be sent for any bulk events or poses. Any Pulsar functions that are tracking the event log will not run for these events. Single instance Multiple instances POST /api/events { \"start_datetime\": <ISO_datetime>, \"start_pose\": <pose_url>, \"event_type\": <event_type_url>, \"metadata\": <metadata>, } POST /api/events [ { \"start_datetime\": <ISO_datetime>, \"start_pose\": <pose_url>, \"event_type\": <event_type_url>, \"metadata\": <metadata>, }, { \"start_datetime\": <ISO_datetime>, \"start_pose\": <pose_url>, \"event_type\": <event_type_url>, \"metadata\": <metadata>, }, { \"start_datetime\": <ISO_datetime>, \"start_pose\": <pose_url>, \"event_type\": <event_type_url>, \"metadata\": <metadata>, } ]","title":"API Usage"},{"location":"usage.html#api-usage","text":"For the API during an active test mission, we'll generally be concerned with creating one of the following: Poses , Events , Notes , Images . As opposed to the configuration which was largely done through the Django custom command, these will be created through their respective API endpoints.","title":"API Usage"},{"location":"usage.html#poses","text":"Poses can be used to track entities during each test run. Here is an example of posting a pose. Minimal Comprehensive import datetime import requests pose_data = { \"lat\" : 32.67 , \"lon\" : - 117.24 , \"entity\" : \"/api/entities/alpha_1/\" , \"pose_source\" : \"/api/pose_sources/1/\" , \"timestamp\" : datetime . datetime . now () . isoformat (), \"trial\" : 1 , } requests . post ( \"http://localhost/api/poses/\" , json = pose_data , auth = ( \"admin\" , \"admin\" )) import datetime import requests pose_data = { \"lat\" : 32.67 , \"lon\" : - 117.24 , \"entity\" : \"/api/entities/alpha_1/\" , \"pose_source\" : \"/api/pose_sources/1/\" , \"timestamp\" : datetime . datetime . now () . isoformat (), \"trial\" : 1 , \"elevation\" : 0.0 , \"heading\" : 23.0 , \"speed\" : 23 , \"velocity\" : [ 133.0 , 2.0 ], } requests . post ( \"http://localhost/api/poses/\" , json = pose_data , auth = ( \"admin\" , \"admin\" )) entity is the entity that this pose is for. pose_source indicates what type of pose it is. This string should contain the id of a pose source. timestamp represents the datetime at which this pose happened. trial indicates which specific test run you want to associate this pose with. The same entity can have different paths through the test environment so this isolates the poses to their respective trial. This field would be assigned the trial id. elevation is an optional arbitrary float value. This could be used to represent meters from sea level or levels in a building. heading is an optional arbitrary float value. A good rule of thumb is to use 0.0 and/or 360.0 as north and go clockwise (i.e. east would be 90.0 ) speed and velocity are both optional fields. velocity is an array, allowing to provide either 2D or 3D vectors. speed isn't automatically calculated so you'll have to manually set the scalar value. There is no validation that the speed field is the correct speed for the velocity field.","title":"Poses"},{"location":"usage.html#events","text":"Events will be the primary mode of recording data. These will use the EventTypes created during the configuration and can include any other specific data and/or metadata that you wish to attach. The following is an example of posting an event. Minimal Comprehensive import datetime import requests json_to_post = { \"start_datetime\" : datetime . datetime . now () . isoformat (), \"event_type\" : \"/api/event_types/1/\" , \"trial\" : \"/api/trials/1/\" , } requests . post ( \"http://<Mole_IP>/api/events/\" , json = json_to_post , auth = ( \"admin\" , \"admin\" )) import datetime import requests pose_data = { \"lat\" : 32.67 , \"lon\" : - 117.24 , \"entity\" : \"/api/entities/alpha_1/\" , \"pose_source\" : \"/api/pose_sources/1/\" , \"timestamp\" : datetime . datetime . now () . isoformat (), \"trial\" : 1 , } pose_post = requests . post ( \"http://localhost/api/poses/\" , json = pose_data , auth = ( \"admin\" , \"admin\" )) pose_url = pose_post . headers [ 'Location' ] json_to_post = { \"start_datetime\" : datetime . datetime . now () . isoformat (), \"end_datetime\" : \"2022-01-01T08:00:00+00:00\" , \"event_type\" : \"/api/event_types/1/\" , \"trial\" : \"/api/trials/1/\" , \"start_pose\" : pose_url , \"metadata\" : { \"entity\" : \"usv1\" , \"extra_info\" : 25.2 , \"list_of_important_things\" : [ \"item1\" , \"item2\" , ], } } requests . post ( \"http://<Mole_IP>/api/events/\" , json = json_to_post , auth = ( \"admin\" , \"admin\" )) start_datetime and end_datetime should be a string in the format of ISO 8601. start_datetime is a required field. end_datetime is optional and intended for events with a duration of some kind. It can be omitted and added later or omitted entirely. event_type should be a string with the corresponding id for the event type you want to create. trial should be a string with the corresponding id for the trial you want to attach this event to. start_pose is an optional field that ties a pose to this event. Its value should be the url of a previously created pose. Currently, it is not possible to create both a pose and event in the same HTTP request. You can use this start pose field to indicate something like an interaction happening at a specific location. metadata should be a Python dictionary of relevent data and metadata about the event. If not supplied, it will default to an empty dictionary. The target url should be whatever IP the Mole server is hosted on. This could be an IP or http://localhost if posting locally from the same host. Note The crendentials used are based off the example configuration in the basic configuration . If these have changed, make sure to use the correct username and password.","title":"Events"},{"location":"usage.html#notes","text":"Notes provide an area to supplement events with free-form notes. All notes must be attached to an event. import datetime import requests json_to_post = { \"tester\" : \"/api/testers/1/\" , \"note\" : \"Test note here\" , \"event\" : \"/api/events/1/\" , } requests . post ( \"http://localhost/api/notes/\" , json = json_to_post , auth = ( \"admin\" , \"admin\" ))","title":"Notes"},{"location":"usage.html#images","text":"Images can be useful for traceability and debugging. You can attach an image(s) to any event. import datetime import requests files = { \"image\" : open ( '/directory/to/image.png' , 'rb' ), } data = { \"image_type\" : \"/api/image_types/1/\" , \"event\" : \"/api/events/1/\" , \"timestamp\" : datetime . datetime . now () . isoformat (), } r = requests . post ( \"http://localhost/api/images/\" , files = files , data = data , auth = ( \"admin\" , \"admin\" ))","title":"Images"},{"location":"usage.html#bulk-posts","text":"Mole also has the ability to create multiple instances for a single HTTP POST request. Currently this ability only exists for events and poses. To use this ability, simply pass a list of objects rather than a single object. This feature is useful if single posts are too slow for the level of throughput desired and increasing the number of Gunicorn/Django workers is not possible. Testing might be required to determine the optimal number of objects to post during each request. Also note that Pulsar messages will not be sent for any bulk events or poses. Any Pulsar functions that are tracking the event log will not run for these events. Single instance Multiple instances POST /api/events { \"start_datetime\": <ISO_datetime>, \"start_pose\": <pose_url>, \"event_type\": <event_type_url>, \"metadata\": <metadata>, } POST /api/events [ { \"start_datetime\": <ISO_datetime>, \"start_pose\": <pose_url>, \"event_type\": <event_type_url>, \"metadata\": <metadata>, }, { \"start_datetime\": <ISO_datetime>, \"start_pose\": <pose_url>, \"event_type\": <event_type_url>, \"metadata\": <metadata>, }, { \"start_datetime\": <ISO_datetime>, \"start_pose\": <pose_url>, \"event_type\": <event_type_url>, \"metadata\": <metadata>, } ]","title":"Bulk POSTs"},{"location":"utilities_event_mocker.html","text":"Event Mocker \u00b6 The Event Mocker is a tool used to create and post random and/or sequenced events to Mole. There are two different ways of using the event mocker: Random Event Mocking and Event Sequence Mocking . Random Event Mocking is useful for: load testing quickly getting events in the database Event Sequence Mocking is useful for: testing behavior when certain events happen in sequence simulating a realistic trial Configuration \u00b6 The first step is to ensure the python requests library is installed on your machine. pip install requests The script requires at least one configuration file passed as an argument. Below is an example of a configuration file that meets the requirements for both Random Event Mocking and Event Sequence Mocking. { \"username\": \"admin\", \"password\": \"admin\", \"trial_duration\": { \"hours\": 1, \"minutes\": 0 }, \"bounding_box\": { \"min_point\": { \"lat\": 32.657360, \"lon\": -117.283920 }, \"max_point\": { \"lat\": 32.740647, \"lon\": -117.157868 } }, \"event_metadata\": { \"detail\": [\"Event created by Mole Utilities Event Generator.\"], \"key_1\": [\"choice_1\", \"choice_2\", \"choice_3\"], \"key_2\": [ \"single_choice\", [1, 2, 3], [\"a\", \"b\", \"c\"] ], \"key_n\": [ {\"nested_1\": \"1a\"}, {\"nested_2\": \"2b\"}, {\"nested_3\": \"3c\"} ] }, \"event_type_metadata\": { \"Node Online\": { \"node\": [\"node-1\", \"node-2\", \"node-3\"] } }, \"event_count\": 100, \"sequence_order\": [ [\"Start\", 1], [\"RANDOM\", 50], [\"End\", 1] ], \"sequences\": { \"Start\": [ [\"Node Online\", 50], [\"Trial Start\", 1] ], \"End\": [ [\"Trial End\", 1], [\"Safety Stop\", 1] ] }, \"excluded_types\": [ \"Unassigned\", \"Ignore\" ], } Required Keys: username : string . Admin username. password : string . Admin password. trial_duration : dict . Window of time for events to occur in. hours : int . Number of hours for the current trial. minutes : int . Number of minutes for the current trial. bounding_box : dict . Geographical area for events to be plotted in. min_point : dict . Bottom left corner of area. lat : float . Latitude value for bottom left point. lon : float . Longitude value for bottom left point. max_point : dict . Top right corner of area. lat : float . Latitude value for top right point. lon : float . Longitude value for top right point. event_metadata : dict, may be empty . Metadata for every event. Takes a dictionary, value for each key must be an array. The generator will randomly select one value from the provided array for each provided key. event_type_metadata : dict, may be empty . Metadata for specifc event types. Takes a dictionary, each key is the name of the event type. The value for each key is the metadata dictionary. Value for each metadata key must be an array. The generator will randomly select one value from the provided array for each provided key. Keys Relevant for Random Event Mocking: event_count : int . The number of events you would like to generate. Note event_count will be overriden if the -c tag is used in the command line argument. Keys Relevant for Event Sequence Mocking: sequence_order : array . a list of tuples. The first element of the tuple is the name of the user-defined sequence. The second element is how many times you would like to generate that sequence. If you would like to generate random events within the sequence, use the pre-defined sequence name RANDOM with the number of random events to generate. Sequences will be generated in order of array. sequences : dict . A dictionary of user-defined sequences. The key is the name of your sequence. The value is an array of tuples. The value of the first tuple element is the event type name. The value of the second element is the number of times that event type should occur. Events will be generated in order of array. Optional: excluded_types : array, optional . An array of event types to exclude when mocking random events. Run \u00b6 The script requires one configuration file that provides the required keys, or multiple configuration files that, when composed, provide the required keys. If you include multiple configuration files, key conflicts will be handled by choosing the value from the latest file. Random Event Mocking From the /utilities directiory, run: python mock_events.py path/to/your/config.json path/to/another/config.json Tip To override the number of random events to mock, add tag -c followed by the number of events. Example: python mock_events.py -c 100 path/to/your/config.json Event Sequence Mocking From the /utilities directiory, run: python mock_events.py -seq path/to/your/config.json path/to/another/config.json","title":"Event Mocker"},{"location":"utilities_event_mocker.html#event-mocker","text":"The Event Mocker is a tool used to create and post random and/or sequenced events to Mole. There are two different ways of using the event mocker: Random Event Mocking and Event Sequence Mocking . Random Event Mocking is useful for: load testing quickly getting events in the database Event Sequence Mocking is useful for: testing behavior when certain events happen in sequence simulating a realistic trial","title":"Event Mocker"},{"location":"utilities_event_mocker.html#configuration","text":"The first step is to ensure the python requests library is installed on your machine. pip install requests The script requires at least one configuration file passed as an argument. Below is an example of a configuration file that meets the requirements for both Random Event Mocking and Event Sequence Mocking. { \"username\": \"admin\", \"password\": \"admin\", \"trial_duration\": { \"hours\": 1, \"minutes\": 0 }, \"bounding_box\": { \"min_point\": { \"lat\": 32.657360, \"lon\": -117.283920 }, \"max_point\": { \"lat\": 32.740647, \"lon\": -117.157868 } }, \"event_metadata\": { \"detail\": [\"Event created by Mole Utilities Event Generator.\"], \"key_1\": [\"choice_1\", \"choice_2\", \"choice_3\"], \"key_2\": [ \"single_choice\", [1, 2, 3], [\"a\", \"b\", \"c\"] ], \"key_n\": [ {\"nested_1\": \"1a\"}, {\"nested_2\": \"2b\"}, {\"nested_3\": \"3c\"} ] }, \"event_type_metadata\": { \"Node Online\": { \"node\": [\"node-1\", \"node-2\", \"node-3\"] } }, \"event_count\": 100, \"sequence_order\": [ [\"Start\", 1], [\"RANDOM\", 50], [\"End\", 1] ], \"sequences\": { \"Start\": [ [\"Node Online\", 50], [\"Trial Start\", 1] ], \"End\": [ [\"Trial End\", 1], [\"Safety Stop\", 1] ] }, \"excluded_types\": [ \"Unassigned\", \"Ignore\" ], } Required Keys: username : string . Admin username. password : string . Admin password. trial_duration : dict . Window of time for events to occur in. hours : int . Number of hours for the current trial. minutes : int . Number of minutes for the current trial. bounding_box : dict . Geographical area for events to be plotted in. min_point : dict . Bottom left corner of area. lat : float . Latitude value for bottom left point. lon : float . Longitude value for bottom left point. max_point : dict . Top right corner of area. lat : float . Latitude value for top right point. lon : float . Longitude value for top right point. event_metadata : dict, may be empty . Metadata for every event. Takes a dictionary, value for each key must be an array. The generator will randomly select one value from the provided array for each provided key. event_type_metadata : dict, may be empty . Metadata for specifc event types. Takes a dictionary, each key is the name of the event type. The value for each key is the metadata dictionary. Value for each metadata key must be an array. The generator will randomly select one value from the provided array for each provided key. Keys Relevant for Random Event Mocking: event_count : int . The number of events you would like to generate. Note event_count will be overriden if the -c tag is used in the command line argument. Keys Relevant for Event Sequence Mocking: sequence_order : array . a list of tuples. The first element of the tuple is the name of the user-defined sequence. The second element is how many times you would like to generate that sequence. If you would like to generate random events within the sequence, use the pre-defined sequence name RANDOM with the number of random events to generate. Sequences will be generated in order of array. sequences : dict . A dictionary of user-defined sequences. The key is the name of your sequence. The value is an array of tuples. The value of the first tuple element is the event type name. The value of the second element is the number of times that event type should occur. Events will be generated in order of array. Optional: excluded_types : array, optional . An array of event types to exclude when mocking random events.","title":"Configuration"},{"location":"utilities_event_mocker.html#run","text":"The script requires one configuration file that provides the required keys, or multiple configuration files that, when composed, provide the required keys. If you include multiple configuration files, key conflicts will be handled by choosing the value from the latest file. Random Event Mocking From the /utilities directiory, run: python mock_events.py path/to/your/config.json path/to/another/config.json Tip To override the number of random events to mock, add tag -c followed by the number of events. Example: python mock_events.py -c 100 path/to/your/config.json Event Sequence Mocking From the /utilities directiory, run: python mock_events.py -seq path/to/your/config.json path/to/another/config.json","title":"Run"},{"location":"utilities_profiler.html","text":"Profiler \u00b6 Mole includes the Silk profiling tool that can be run with the --profile flag on the ./ml init and ./ml run commands. When Mole is run with the --profile flag, Silk is available at http://localhost/silk/ or http://localhost:8000/silk/ Note It is not recommended to run the Silk profiler in an operational context. From the project's documentation: Silk is a live profiling and inspection tool for the Django framework. Silk intercepts and stores HTTP requests and database queries before presenting them in a user interface for further inspection. It records things like: Time taken Num. queries Time spent on queries Request/Response headers Request/Response bodies SQL Query Inspection \u00b6 Silk also intercepts SQL queries that are generated by each request. We can get a summary on things like the tables involved, number of joins and execution time (the table can be sorted by clicking on a column header) Function Profiling \u00b6 Silk can also be used to profile specific blocks of code/functions. It provides a decorator and a context manager for this purpose. Silk has an option to generate binary .prof files for more in depth profiling information. These files can be viewed in tools like snakeviz or other cProfile tools. This option is off by default in Mole because it rapidly fills the mole/media/ directory with .prof files. It can be enabled by uncommenting the line # SILKY_PYTHON_PROFILER_BINARY = True in mole/mole/settings/settings.py . Tip For more information, see the Silk documentation","title":"**Profiler**"},{"location":"utilities_profiler.html#profiler","text":"Mole includes the Silk profiling tool that can be run with the --profile flag on the ./ml init and ./ml run commands. When Mole is run with the --profile flag, Silk is available at http://localhost/silk/ or http://localhost:8000/silk/ Note It is not recommended to run the Silk profiler in an operational context. From the project's documentation: Silk is a live profiling and inspection tool for the Django framework. Silk intercepts and stores HTTP requests and database queries before presenting them in a user interface for further inspection. It records things like: Time taken Num. queries Time spent on queries Request/Response headers Request/Response bodies","title":"Profiler"},{"location":"utilities_profiler.html#sql-query-inspection","text":"Silk also intercepts SQL queries that are generated by each request. We can get a summary on things like the tables involved, number of joins and execution time (the table can be sorted by clicking on a column header)","title":"SQL Query Inspection"},{"location":"utilities_profiler.html#function-profiling","text":"Silk can also be used to profile specific blocks of code/functions. It provides a decorator and a context manager for this purpose. Silk has an option to generate binary .prof files for more in depth profiling information. These files can be viewed in tools like snakeviz or other cProfile tools. This option is off by default in Mole because it rapidly fills the mole/media/ directory with .prof files. It can be enabled by uncommenting the line # SILKY_PYTHON_PROFILER_BINARY = True in mole/mole/settings/settings.py . Tip For more information, see the Silk documentation","title":"Function Profiling"}]}